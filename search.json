[
  {
    "objectID": "posts/flash-bwd-30-3-2025-kernelized/index.html",
    "href": "posts/flash-bwd-30-3-2025-kernelized/index.html",
    "title": "FlashAttention Kernel: Backward Pass (MATH)",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{s_2 s_1}\\) where \\(s_2\\) and \\(s_1\\) are the index set, for example in a 5D tensor \\(A \\in \\mathbb{R}^{ijklm}\\) a possible index set could be \\(s_2 = \\{ i, j \\}\\) and \\(s_1 = \\{k, l, m \\}\\).\n\\(B_{s_2 s_1} = flash_{s_1}(Q_{s_2 s_1}, K_{s_3 s_1}, V_{s_3 s_1})\\) is a flash attention operation over s1 index set of tensor set \\(&lt;Q, K, V&gt;\\), and the resulting index set is the index set of \\(Q\\) s.t. \\(B \\in \\mathbb{R}^{s_2 s_1}\\)"
  },
  {
    "objectID": "posts/flash-bwd-30-3-2025-kernelized/index.html#preliminary",
    "href": "posts/flash-bwd-30-3-2025-kernelized/index.html#preliminary",
    "title": "FlashAttention Kernel: Backward Pass (MATH)",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{s_2 s_1}\\) where \\(s_2\\) and \\(s_1\\) are the index set, for example in a 5D tensor \\(A \\in \\mathbb{R}^{ijklm}\\) a possible index set could be \\(s_2 = \\{ i, j \\}\\) and \\(s_1 = \\{k, l, m \\}\\).\n\\(B_{s_2 s_1} = flash_{s_1}(Q_{s_2 s_1}, K_{s_3 s_1}, V_{s_3 s_1})\\) is a flash attention operation over s1 index set of tensor set \\(&lt;Q, K, V&gt;\\), and the resulting index set is the index set of \\(Q\\) s.t. \\(B \\in \\mathbb{R}^{s_2 s_1}\\)"
  },
  {
    "objectID": "posts/flash-bwd-30-3-2025-kernelized/index.html#attention-operation",
    "href": "posts/flash-bwd-30-3-2025-kernelized/index.html#attention-operation",
    "title": "FlashAttention Kernel: Backward Pass (MATH)",
    "section": "Attention Operation",
    "text": "Attention Operation\nFor this exercise we will simplify the target index sets to match the most common setup, i.e. \\(Q_{s_2 s_1} \\in \\mathbb{R}^{[M \\times d]}\\), \\(K_{s_3 s_1} \\in \\mathbb{R}^{[N \\times d]}\\), \\(V_{s_3 s_1} \\in \\mathbb{R}^{[N \\times d]}\\)\n\nNote: In this doc, whenever we have to denote the exact dimensions instead of index set it will be denoted as \\(.^{[... \\times ... \\times \\dots]}\\) where \\(\\times\\) symbol separates across different index sets.\n\nThus our operation becomes:\n\\[\\begin{align}\nB_{s_2 s_1} = attention_{s_1}(Q_{s_2 s_1}, K_{s_3 s_1}, V_{s_3 s_1}) \\\\\nB^{[M \\times d]} = attention_{d}(Q^{[M \\times d]}, K^{[N \\times d]}, V^{[N \\times d]})\n\\end{align}\\]\n\n\\[\\begin{align}\nS^{[M \\times N]} = Q @ K^T = Q *_{(M \\times d, N \\times d, M \\times N)} K \\in \\mathbb{R}^{[M \\times N]} \\\\\nS_{rmax}^{M} = \\max_{N} S^{[M \\times N]} \\in \\mathbb{R}^{M} \\\\\nS_{rm}^{[m, n]} = S^{[m, n]} - S_{rmax}^{m} \\forall [m, n] \\in [M \\times N]  \\\\\nP^{[M \\times N]} = softmax_{N}(S_{rm}^{[M \\times N]}) \\\\\nO^{[M \\times d]} = P @ V = P *_{(M \\times N, N \\times d, M \\times d)} V \\in \\mathbb{R}^{[M \\times d]}\n\\end{align}\\]\n\nNote: Notation abuse -&gt; \\(O \\iff B\\)\n\nFor detailed Forward pass derivation please refer to my previous blog: FlashAttention Kernel: Forward Pass (MATH)"
  },
  {
    "objectID": "posts/flash-bwd-30-3-2025-kernelized/index.html#backward-mode-autodiff-pass",
    "href": "posts/flash-bwd-30-3-2025-kernelized/index.html#backward-mode-autodiff-pass",
    "title": "FlashAttention Kernel: Backward Pass (MATH)",
    "section": "Backward (mode autodiff) Pass:",
    "text": "Backward (mode autodiff) Pass:\n\\[\\begin{align}\nB_{s_2 s_1} \\iff B^{[M \\times d]} = attention_{d}(Q^{[M \\times d]}, K^{[N \\times d]}, V^{[N \\times d]})\n\\end{align}\\]\nFor a given loss value \\(O_{s_3 = \\phi}\\) and known \\(dB_{\\phi s_2 s_1} = \\frac{dB_{s_2 s_1}}{dO_{s_3 = \\phi}}\\) We need to find out \\(dQ_{\\phi s_2 s_1}\\), \\(dK_{\\phi s_3 s_1}\\), and \\(dV_{\\phi s_3 s_1}\\).\nHere we will directly differentiate the core attention operation without adjusting for numerical stability of exponent (we did so in forward pass to just make computation stable). Here we will first derive the core backward operations and then change it to computation, followed by mitigating any source of numerical instability.\n\n\\(dV_{\\phi s_3 s_1}\\):\nConsider following op: \\(B_{s_2 s_1} = \\sum_{s_3} P_{s_2 s_3} \\cdot V_{s_3 s_1} = P^{[M \\times N]} @ V^{[N \\times d]} \\in \\mathbb{R}^{[M \\times d]}_{s_2 s_1}\\)\n\\[\\begin{align}\ndV_{\\phi s'_3 s'_1} = \\frac{\\partial O_{\\phi}}{\\partial V_{s'_3 s'_1}} = \\sum_{s_2 s_1} \\frac{\\partial O_{\\phi}}{\\partial B_{s_2 s_1}} \\frac{\\partial B_{s_2 s_1}}{\\partial V_{s'_3 s'_1}} = \\sum_{s_2 s_1} dB_{s_2 s_1} \\frac{\\partial B_{s_2 s_1}}{\\partial V_{s'_3 s'_1}} \\\\\n\\frac{\\partial B_{s_2 s_1}}{\\partial V_{s'_3 s'_1}} = \\sum_{s_3} P_{s_2 s_3} \\mathbb{1}_{(s_3 s_1) = (s'_3 s'_1)} = P_{s_2 s'_3} \\mathbb{1}_{s_1 = s'_1} \\\\\n\\sum_{s_2 s_1} dB_{s_2 s_1} \\frac{\\partial B_{s_2 s_1}}{\\partial V_{s'_3 s'_1}} = \\sum_{s_2 s_1} dB_{s_2 s_1} P_{s_2 s'_3} \\mathbb{1}_{s_1 = s'_1} = \\sum_{s_2} dB_{s_2 s'_1} P_{s_2 s'_3} \\\\\ndV_{\\phi s'_3 s'_1} = \\sum_{s_2 s_1} dB_{s_2 s_1} \\frac{\\partial B_{s_2 s_1}}{\\partial V_{s'_3 s'_1}} = \\sum_{s_2} dB_{s_2 s'_1} P_{s_2 s'_3} = dB^T \\cdot P\n\\end{align}\\]\n\nNote: This also provides a crucial propetry of tensor differentiation i.e. for tensor product operation \\(C_{s_2 s_3} = \\sum_{s_1} A_{s_2 s_1} B_{s_1 s_3} = A \\cdot B\\) then for a given \\(dC_{s_o s_2 s_3}\\) the derivative \\(dA_{s_o s'_2 s'_1} = \\sum_{s_3} dC_{s_o s'_2 s_3} B_{s'_1 s_3}\\) for a simple matmul i.e. \\(s_o = \\phi, s_3 \\in \\mathbb{R}, s_2 \\in \\mathbb{R}, s_1 \\in \\mathbb{R}\\) this operation shrinks to simply \\(dA = dC \\cdot B^T\\). Similarly for \\(dB_{s_o s'_1 s'_3} = \\sum_{s_2} dC_{s_o s_2 s'_3} A_{s_2 s'_1}\\) for a simple matrix multiplication this would reduce to \\(dB = dC^T \\cdot A\\)\n\n\n\n\\(dP_{\\phi s_2 s_3}\\):\nFrom the formula derived previously \\(dP_{\\phi s'_2 s'_3} = \\sum_{s_1} dB_{s'_2 s_1} V_{s'_3 s_1} = dB \\cdot V^T\\)\n\n\n\\(dQK^T_{\\phi s_2 s_3}\\)\nHere we have encountered softmax operation as \\(P_{s_2 s_3} = softmax_{s_3}(S_{s_2 s_3} = QK^T_{s_2 s_3})\\) from the softmax blog we can\n\nDirect Operation:\n\\[\\begin{equation}\nB_{s_2 s_1} = softmax_{s_1}(A_{s_2 s_1})\n\\end{equation}\\]\nHere \\(O_{s_3}\\) is the final loss value for which we need to extract the derivatives.\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}} = B_{s'_2 s'_1} \\left[dB_{s_3 s'_2 s'_1} - \\sum_{s_1} dB_{s_3 s'_2 s_1} B_{s'_2 s_1}\\right]\n\\end{align}\\]\n\n\nFrom the formula, we can say:\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial QK^T_{s'_2 s'_3}} = P_{s'_2 s'_3} \\left[dP_{\\phi s'_2 s'_3} -  \\sum_{s_3} dP_{\\phi s'_2 s_3} P_{s'_2 s_3}\\right]\n\\end{align}\\]\nfor a simple matmul:\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial S_{i'j'}} = P_{i'j'} \\left[dP_{i'j'} - dP_{i':}^T \\circ P_{i':} \\right] = P \\times \\left[dP - BMM(dP_{i'1j, P_{i'j1}}) \\right]\n\\end{align}\\]\n\n\n\n\\(dQ_{\\phi s_2 s_1}\\) & \\(dK_{\\phi s_2 s_1}\\):\n\\(S = QK^T\\) and we know \\(dS\\) thus we can directly write the derivatives of both \\(Q\\) and \\(K\\).\n\\[\\begin{align}\ndQ = dS \\cdot K \\\\\ndK = dS^T \\cdot Q\n\\end{align}\\]\n\n\nFinal Backward Pass Equations:\n\\[\\begin{align}\ndB \\in \\mathbb{R}^{[M \\times D]}, \\{Q, dQ\\} \\in \\mathbb{R}^{[M \\times D]}, \\\\ \\{K, dK\\} \\in \\mathbb{R}^{[N \\times D]}, \\{V, dV\\} \\in \\mathbb{R}^{[N \\times D]}\\\\\n\\end{align}\\]\n\n\\[\\begin{align}\ndV = dB^T \\cdot P \\\\\ndP = dB \\cdot V^T \\\\\ndS_{i'j'} = P_{i'j'} \\left[dP_{i'j'} - dP_{i':}^T \\circ P_{i':} \\right] \\\\\ndQ = dS \\cdot K \\\\\ndK = dS^T \\cdot Q\n\\end{align}\\]\n\nExpansion in dimensions:\n\\[\\begin{align}\nS_{ij} = \\sum_d q_{i d} k_{j d} \\\\\ndV_{j d} = dB^T \\cdot P = \\sum_i dB_{i d} P_{i j} = \\sum_i dB_{i d} \\frac{\\exp(S_{i j})}{L_i} \\\\\ndP_{i j} = dB \\cdot V^T = \\sum_d dB_{i d} V_{j d} \\\\\ndS_{i j} = P_{i j} \\left[dP_{i j} - \\sum_j dP_{i j} P_{i j} \\right] \\\\\ndQ_{i d} = dS \\cdot K = \\sum_{j} dS_{i j} K_{j d}\\\\\ndK_{j d} = dS^T \\cdot Q = \\sum_{i} dS_{i j} Q_{i d}\n\\end{align}\\]\n\n\nAbstract away D dimension:\n\nIn a future blog we will see that this is natural to do computation along D (embedding) dimension as all of the computations are independent of each other in this dimension.\n\n\\[\\begin{align}\nS_{ij} = q_i \\circ k_j \\\\\ndV_j = \\sum_i dB_{i d} \\frac{\\exp(q_i \\circ k_j)}{L_i} \\\\\ndP_{i j} = dB_i \\circ V_j \\\\\ndS_{i j} = P_{i j} \\left[dP_{i j} - \\sum_j dP_{i j} P_{i j} \\right] \\\\\ndQ_i = dS_i \\circ K_j \\\\\ndK_j = dS^T_j \\circ Q_i \\\\\n\\end{align}\\]\n\n\\[\\begin{align}\n\\sum_j dP_{i j} P_{i j} = \\sum_j \\big(\\sum_d dB_{i d} V_{j d}\\big) P_{i j} = \\sum_j \\sum_d dB_{i d} V_{j d} P_{i j} \\\\\n= \\sum_d \\sum_j dB_{i d} V_{j d} P_{i j} = \\sum_d dB_{i d} \\sum_j V_{j d} P_{i j} = \\sum_d dB_{i d} B_{i d}\n\\end{align}\\]\n\\[\\begin{align}\nS_{ij} = q_i \\circ k_j \\\\\ndV_j = \\sum_i dB_{i d} \\frac{\\exp(q_i \\circ k_j)}{L_i} \\\\\ndP_{i j} = dB_i \\circ V_j \\\\\ndS_{i j} = P_{i j} \\left[dP_{i j} - dB_i \\circ B_i \\right] \\\\\ndQ_i = dS_i \\circ K_j \\\\\ndK_j = dS^T_j \\circ Q_i \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "posts/softmax-30-3-2025-kernelized/index.html",
    "href": "posts/softmax-30-3-2025-kernelized/index.html",
    "title": "SoftMax Kernel: Forward and Backward Pass (MATH)",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{s_2 s_1}\\) where \\(s_2\\) and \\(s_1\\) are the index set, for example in a 5D tensor \\(A \\in \\mathbb{R}^{ijklm}\\) a possible index set could be \\(s_2 = \\{ i, j \\}\\) and \\(s_1 = \\{k, l, m \\}\\).\n\\(B_{s_2 s_1} = softmax_{s_1}(A_{s_2 s_1})\\) is a softmax operation over s1 index set of A, and the resulting index set still remains same as A s.t. \\(B \\in \\mathbb{R}^{s2s1}\\)"
  },
  {
    "objectID": "posts/softmax-30-3-2025-kernelized/index.html#softmax-operation",
    "href": "posts/softmax-30-3-2025-kernelized/index.html#softmax-operation",
    "title": "SoftMax Kernel: Forward and Backward Pass (MATH)",
    "section": "Softmax Operation",
    "text": "Softmax Operation\n\\[\\begin{equation}\nB_{s_2 s_1} = softmax_{s_1}(A_{s_2 s_1})\n\\end{equation}\\]\n\nIntermediate Result: #1\n\\[\\begin{equation}\nI^1_{s_2 s_1} = \\exp{(A_{s_2 s_1})}\n\\end{equation}\\]\n\n\nIntermediate Result: #2\n\\[\\begin{equation}\nI^2_{s_2} = \\sum_{s1}I^1_{s_2 s_1}\n\\end{equation}\\]\n\n\nSoftmax:\n\\[\\begin{equation}\nB_{s_2 s_1} = \\frac{I^1_{s_2 s_1}}{I^2_{s_2}} = \\frac{\\exp{(A_{s_2 s_1})}}{\\sum_{s1}\\exp{(A_{s_2 s_1})}}\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/softmax-30-3-2025-kernelized/index.html#backward-pass",
    "href": "posts/softmax-30-3-2025-kernelized/index.html#backward-pass",
    "title": "SoftMax Kernel: Forward and Backward Pass (MATH)",
    "section": "Backward Pass:",
    "text": "Backward Pass:\nHere we will just look at the backward pass of the softmax kernel alone, as it will help us understand a much wider concept of having a multidimensional loss function instead of a scalar loss.\nFortunately this also simplifies the problem for us as we won’t have to account for any pullbacks for the output \\(B_{s_2 s_1}\\) itself.\n\\[\\begin{equation}\n\\frac{\\partial B_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} \\in \\mathbb{R}^{s_2 s_1 s'_2 s'_1}\n\\end{equation}\\]\n\nDerivation:\n\\[\\begin{align}\n\\frac{\\partial B_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} = \\frac{1}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\frac{\\partial \\exp{(A_{s_2 s_1})}}{\\partial A_{s'_2 s'_1}} + \\exp(A_{s_2 s_1})\\frac{\\partial \\sum_{s_1} 1/exp(A_{s_2 s_1})}{\\partial A_{s'_2 s'_1}}\n\\end{align}\\]\n\n\\[\\begin{align}\n\\frac{1}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\frac{\\partial \\exp{(A_{s_2 s_1})}}{\\partial A_{s'_2 s'_1}} = \\frac{\\exp(A_{s_2 s_1})}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\frac{A_{s_2 s_1}}{A_{s'_2 s'_1}} = \\frac{\\exp(A_{s_2 s_1})}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)}\n\\end{align}\\]\n\n\\[\\begin{equation}\n\\exp(A_{s_2 s_1})\\frac{\\partial \\sum_{s_1} 1/exp(A_{s_2 s_1})}{\\partial A_{s'_2 s'_1}} = -\\frac{\\exp(A_{s_2 s_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\frac{\\partial \\sum_{s_1} \\exp(A_{s_2 s_1})}{\\partial A_{s'_2 s'_1}}\n\\end{equation}\\]\n\\[\\begin{align}\n\\frac{\\partial \\sum_{s_1} \\exp(A_{s_2 s_1})}{\\partial A_{s'_2 s'_1}} = \\sum_{s_1} \\frac{\\partial \\exp(A_{s_2 s_1})}{\\partial A_{s'_2 s'_1}} = \\sum_{s_1} \\exp(A_{s_2 s_1}) \\frac{\\partial A_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} \\\\ = \\sum_{s_1} \\exp(A_{s_2 s_1}) \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} = \\exp(A_{s_2 s'_1}) \\mathbb{1}_{s_2 = s'_2}\n\\end{align}\\]\n\n\nFinal Derivative Simplification:\n\n\\[\\begin{align}\n\\frac{\\partial B_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} = \\frac{\\exp(A_{s_2 s_1})}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} - \\frac{\\exp(A_{s_2 s_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\exp(A_{s_2 s'_1}) \\mathbb{1}_{s_2 = s'_2} \\\\ = \\frac{\\exp(A_{s_2 s_1})}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} - \\frac{\\exp(A_{s_2 s_1}) \\exp(A_{s_2 s'_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\mathbb{1}_{s_2 = s'_2} \\\\\n= B_{s_2 s_1} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} - B_{s_2 s_1}B_{s_2 s'_1} \\mathbb{1}_{s_2 = s'_2}\n\\end{align}\\]\n\n\n\\[\\begin{align}\n\\frac{\\partial B_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} = B_{s_2 s_1} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} - B_{s_2 s_1}B_{s_2 s'_1} \\mathbb{1}_{s_2 = s'_2}\n\\end{align}\\]\n\n\n\nFull reduction: With Loss Drivative\nLet’s assume the output \\(B_{s_2 s_1}\\) being used by some frisky function to generate loss \\(O_{s_3}\\) and somehow we have the pullback of \\(B\\) w.r.t. \\(O\\) as \\(dB_{s_3 s_2 s_1} = \\frac{\\partial O_{s_3}}{\\partial B_{s_2 s_1}}\\), and now we are interested in find out what’s the pullback of \\(A_{s'_2 s'_1}\\) w.r.t. \\(O_{s_3}\\) i.e. \\(\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}}\\).\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}} = \\sum_{s_2 s_1} \\frac{\\partial O_{s_3}}{\\partial B_{s_2 s_1}}\\frac{\\partial B_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} \\\\\n= \\sum_{s_2 s_1} dB_{s_3 s_2 s_1} \\left[ \\frac{\\exp(A_{s_2 s_1})}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} - \\frac{\\exp(A_{s_2 s_1}) \\exp(A_{s_2 s'_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\mathbb{1}_{s_2 = s'_2} \\right] \\\\\n= dB_{s_3 s'_2 s'_1} \\frac{\\exp(A_{s'_2 s'_1})}{\\sum_{s_1}\\exp{(A_{s'_2 s_1})}} - \\sum_{s_2 s_1} dB_{s_3 s_2 s_1} \\frac{\\exp(A_{s_2 s_1}) \\exp(A_{s_2 s'_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\mathbb{1}_{s_2 = s'_2}\n\\end{align}\\]\n\n\\[\\begin{align}\n\\sum_{s_2 s_1} dB_{s_3 s_2 s_1} \\frac{\\exp(A_{s_2 s_1}) \\exp(A_{s_2 s'_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\mathbb{1}_{s_2 = s'_2} = \\sum_{s_1} dB_{s_3 s'_2 s_1} \\frac{\\exp(A_{s'_2 s_1}) \\exp(A_{s'_2 s'_1})}{[\\sum_{s_1} exp(A_{s'_2 s_1})]^2} \\\\\n= \\frac{\\exp(A_{s'_2 s'_1})}{[\\sum_{s_1} exp(A_{s'_2 s_1})]^2} \\sum_{s_1} [dB_{s_3 s'_2 s_1} \\exp(A_{s'_2 s_1})]\n\\end{align}\\]\n\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}}\n= dB_{s_3 s'_2 s'_1} \\frac{\\exp(A_{s'_2 s'_1})}{\\sum_{s_1}\\exp{(A_{s'_2 s_1})}} - \\frac{\\exp(A_{s'_2 s'_1})}{[\\sum_{s_1} exp(A_{s'_2 s_1})]^2} \\sum_{s_1} [dB_{s_3 s'_2 s_1} \\exp(A_{s'_2 s_1})] \\\\\n= dB_{s_3 s'_2 s'_1} B_{s'_2 s'_1} - B_{s'_2 s'_1}\\frac{1}{\\sum_{s_1} exp(A_{s'_2 s_1})} \\sum_{s_1} [dB_{s_3 s'_2 s_1} \\exp(A_{s'_2 s_1})] \\\\\n= dB_{s_3 s'_2 s'_1} B_{s'_2 s'_1} - B_{s'_2 s'_1} \\sum_{s_1} [dB_{s_3 s'_2 s_1} B_{s'_2 s_1}] = B_{s'_2 s'_1} [dB_{s_3 s'_2 s'_1} - \\sum_{s_1} [dB_{s_3 s'_2 s_1} B_{s'_2 s_1}]]\n\\end{align}\\]\n\n\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}} = B_{s'_2 s'_1} \\left[dB_{s_3 s'_2 s'_1} - \\sum_{s_1} dB_{s_3 s'_2 s_1} B_{s'_2 s_1}\\right]\n\\end{align}\\]"
  },
  {
    "objectID": "posts/softmax-30-3-2025-kernelized/index.html#application-in-attention-specialize-the-expressions",
    "href": "posts/softmax-30-3-2025-kernelized/index.html#application-in-attention-specialize-the-expressions",
    "title": "SoftMax Kernel: Forward and Backward Pass (MATH)",
    "section": "Application in Attention: Specialize the Expressions",
    "text": "Application in Attention: Specialize the Expressions\n\nSetup:\n\\[\\begin{align}\nP_{ij} = softmax(S_{ij}) \\\\\nS_{ij} \\in \\mathbb{R}^{M \\times N} \\\\\nP_{ij} \\in \\mathbb{R}^{M \\times N} \\\\\nO_{\\phi} \\in \\mathbb{R} \\\\\ndP_{\\phi ij} = \\frac{\\partial O_{\\phi}}{\\partial P_{ij}} \\in \\mathbb{R}^{\\phi \\times M \\times N} \\implies \\text{Known}\n\\end{align}\\]\n\n\nDifferentiation:\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial S_{i'j'}} = \\sum_{ij} \\frac{\\partial O_{\\phi}}{\\partial P_{ij}}\\frac{\\partial P_{ij}}{\\partial S_{i'j'}} = P_{i'j'} \\left[dP_{\\phi i'j'} - \\sum_{j} dP_{\\phi i'j} P_{i'j} \\right] \\\\\n= P_{i'j'} \\left[dP_{i'j'} - \\sum_{j} dP_{i'j} P_{i'j} \\right] = P_{i'j'} \\left[dP_{i'j'} - dP_{i':}^T \\circ P_{i':} \\right] \\\\\n= P \\times \\left[dP - BMM(dP_{i'1j, P_{i'j1}}) \\right]\n\\end{align}\\]\n\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial S_{i'j'}} = P_{i'j'} \\left[dP_{i'j'} - dP_{i':}^T \\circ P_{i':} \\right]\n\\end{align}\\]"
  },
  {
    "objectID": "kernelized.html",
    "href": "kernelized.html",
    "title": "KERNELIZED",
    "section": "",
    "text": "Max Kernel: Forward and Backward Pass (MATH)\n\n\n\n\n\n\nMax\n\n\nAutograd\n\n\nMATH\n\n\nTriton\n\n\nCUDA\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nShivam Pandey\n\n\n\n\n\n\n\n\n\n\n\n\nSoftMax Kernel: Forward and Backward Pass (MATH)\n\n\n\n\n\n\nSoftmax\n\n\nAutograd\n\n\nMATH\n\n\nTriton\n\n\nCUDA\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nShivam Pandey\n\n\n\n\n\n\n\n\n\n\n\n\nFlashAttention Kernel: Forward Pass (MATH)\n\n\n\n\n\n\nFlashAttention\n\n\nTransformers\n\n\nAttention\n\n\nAutograd\n\n\nMATH\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nShivam Pandey\n\n\n\n\n\n\n\n\n\n\n\n\nFlashAttention Kernel: Backward Pass (MATH)\n\n\n\n\n\n\nFlashAttention\n\n\nTransformers\n\n\nAttention\n\n\nAutograd\n\n\nMATH\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nShivam Pandey\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shivam Pandey",
    "section": "",
    "text": "Welcome to my site! I’m Shivam Pandey, a tech explorer dedicated to advancing AI’s explainability and its ability to generalize across tasks with minimal data.\n\n\nScaling computes, not comprehension.\n\nEven massive compute and oceans of data only deliver brute-force wins—they scale performance, not true insight. These systems remain data inefficient and fail to generalize, missing the mark on real intelligence.\n\nCurrently I’m working on Quantifying the Intelligence-Compute Decoupling in Modern AI Systems."
  },
  {
    "objectID": "index.html#research-orientation-wip",
    "href": "index.html#research-orientation-wip",
    "title": "Shivam Pandey",
    "section": "",
    "text": "Scaling computes, not comprehension.\n\nEven massive compute and oceans of data only deliver brute-force wins—they scale performance, not true insight. These systems remain data inefficient and fail to generalize, missing the mark on real intelligence.\n\nCurrently I’m working on Quantifying the Intelligence-Compute Decoupling in Modern AI Systems."
  },
  {
    "objectID": "posts/max-30-3-2025-kernelized/index.html",
    "href": "posts/max-30-3-2025-kernelized/index.html",
    "title": "Max Kernel: Forward and Backward Pass (MATH)",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{s_2 s_1}\\) where \\(s_2\\) and \\(s_1\\) are the index set, for example in a 5D tensor \\(A \\in \\mathbb{R}^{ijklm}\\) a possible index set could be \\(s_2 = \\{ i, j \\}\\) and \\(s_1 = \\{k, l, m \\}\\).\n\\(B_{s_2} = max_{s_1}(A_{s_2 s_1})\\) is a max operation over s1 index set of A, and the resulting index set is the index set of \\(A\\) reduced over \\(s_1\\) s.t. \\(B \\in \\mathbb{R}^{s_2}\\)"
  },
  {
    "objectID": "posts/max-30-3-2025-kernelized/index.html#max-reduction-operation",
    "href": "posts/max-30-3-2025-kernelized/index.html#max-reduction-operation",
    "title": "Max Kernel: Forward and Backward Pass (MATH)",
    "section": "Max (Reduction) operation",
    "text": "Max (Reduction) operation\n\\[\\begin{align}\nB_{s_2} = \\max_{s_1}(A_{s_2 s_1}) = A_{s_2 s^m_1} \\big|_{s^m_1 = argmax_{s_1}(A_{s_2 s_1})}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/max-30-3-2025-kernelized/index.html#backward-pass",
    "href": "posts/max-30-3-2025-kernelized/index.html#backward-pass",
    "title": "Max Kernel: Forward and Backward Pass (MATH)",
    "section": "Backward Pass:",
    "text": "Backward Pass:\nHere we will deduce the pullback of \\(A\\) under the \\(\\max\\) operation, w.r.t. \\(B\\).\n\\[\\begin{align}\n\\frac{\\partial B_{s_2}}{\\partial A_{s'_2 s'_1}} = \\frac{\\partial A_{s_2 s^m_1}}{\\partial A_{s'_2 s'_1}} = \\mathbb{1}_{s_2 = s'_2} \\cdot \\mathbb{1}_{s^m_1 = s'_1}\n\\end{align}\\]\n\nFull reduction: With Loss Drivative\nLet’s assume the output \\(B_{s_2}\\) being used by some frisky function to generate loss \\(O_{s_3}\\) and somehow we have the pullback of \\(B\\) w.r.t. \\(O\\) as \\(dB_{s_3 s_2} = \\frac{\\partial O_{s_3}}{\\partial B_{s_2}}\\), and now we are interested in find out what’s the pullback of \\(A_{s'_2 s'_1}\\) w.r.t. \\(O_{s_3}\\) i.e. \\(\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}}\\).\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}} = \\sum_{s_2} \\frac{\\partial O_{s_3}}{\\partial B_{s_2}} \\frac{\\partial B_{s_2}}{\\partial A_{s'_2 s'_1}} = \\sum_{s_2} dB_{s_3 s_2} \\mathbb{1}_{s_2 = s'_2} \\cdot \\mathbb{1}_{s^m_1 = s'_1} = dB_{s_3 s'_2} \\mathbb{1}_{s^m_1 = s'_1} \\big|_{s^m_1 = argmax_{s_1}(A_{s'_2 s_1})}\n\\end{align}\\]\n\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}} = dB_{s_3 s'_2} \\mathbb{1}_{s^m_1 = s'_1} \\big|_{s^m_1 = argmax_{s_1}(A_{s'_2 s_1})}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/max-30-3-2025-kernelized/index.html#application-in-attention-specialize-the-expressions",
    "href": "posts/max-30-3-2025-kernelized/index.html#application-in-attention-specialize-the-expressions",
    "title": "Max Kernel: Forward and Backward Pass (MATH)",
    "section": "Application in Attention: Specialize the Expressions",
    "text": "Application in Attention: Specialize the Expressions\n\nSetup:\n\\[\\begin{align}\nS_{ij} = S^p_{ij} - \\max_j(S^p_{ij}) \\\\\nP_{ij} = softmax(S_{ij}) \\\\\nS_{ij} \\in \\mathbb{R}^{M \\times N} \\\\\nP_{ij} \\in \\mathbb{R}^{M \\times N} \\\\\nO_{\\phi} \\in \\mathbb{R} \\\\\ndP_{\\phi ij} = \\frac{\\partial O_{\\phi}}{\\partial P_{ij}} \\in \\mathbb{R}^{\\phi \\times M \\times N} \\implies \\text{Known} \\\\\ndS_{\\phi ij} = \\frac{\\partial O_{\\phi}}{\\partial S_{ij}} \\in \\mathbb{R}^{\\phi \\times M \\times N} =  P_{ij} \\left[dP_{ij} - dP_{i:}^T \\circ P_{i:} \\right]\n\\end{align}\\]\n\n\nDifferentiation\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial S^p_{i'j'}} = \\sum_{ij} \\frac{\\partial O_{\\phi}}{\\partial S_{ij}} \\frac{\\partial S_{ij}}{\\partial S^p_{i'j'}} = \\sum_{ij} dS_{\\phi ij} (\\mathbb{1}_{ij = i'j'} - \\mathbb{1}_{i = i'} \\mathbb{1}_{j^m = j'} \\big|_{argmax_{j}(S^p_{i'j})}) \\\\\n= dS_{i'j'} - \\sum_{j} dS_{i'j} \\mathbb{1}_{j^m = j'} \\big|_{j^m={argmax_{j}(S^p_{i'j})}} = dS_{i'j'} - \\mathbb{1}_{j^m = j'} \\big|_{j^m={argmax_{j}(S^p_{i'j})}} \\sum_{j} dS_{i'j}\n\\end{align}\\]\n\nReplacing \\(dS_{ij}\\):\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial S^p_{i'j'}} = dS_{i'j'} - \\mathbb{1}_{j^m = j'} \\big|_{j^m={argmax_{j}(S^p_{i'j})}} \\sum_{j} dS_{i'j} \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "posts/flash-30-3-2025-kernelized/index.html",
    "href": "posts/flash-30-3-2025-kernelized/index.html",
    "title": "FlashAttention Kernel: Forward Pass (MATH)",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{s_2 s_1}\\) where \\(s_2\\) and \\(s_1\\) are the index set, for example in a 5D tensor \\(A \\in \\mathbb{R}^{ijklm}\\) a possible index set could be \\(s_2 = \\{ i, j \\}\\) and \\(s_1 = \\{k, l, m \\}\\).\n\\(B_{s_2 s_1} = flash_{s_1}(Q_{s_2 s_1}, K_{s_3 s_1}, V_{s_3 s_1})\\) is a flash attention operation over s1 index set of tensor set \\(&lt;Q, K, V&gt;\\), and the resulting index set is the index set of \\(Q\\) s.t. \\(B \\in \\mathbb{R}^{s_2 s_1}\\)"
  },
  {
    "objectID": "posts/flash-30-3-2025-kernelized/index.html#attention-operation",
    "href": "posts/flash-30-3-2025-kernelized/index.html#attention-operation",
    "title": "FlashAttention Kernel: Forward Pass (MATH)",
    "section": "Attention Operation",
    "text": "Attention Operation\nFor this exercise we will simplify the target index sets to match the most common setup, i.e. \\(Q_{s_2 s_1} \\in \\mathbb{R}^{[M \\times d]}\\), \\(K_{s_3 s_1} \\in \\mathbb{R}^{[N \\times d]}\\), \\(V_{s_3 s_1} \\in \\mathbb{R}^{[N \\times d]}\\)\n\nNote: In this doc, whenever we have to denote the exact dimensions instead of index set it will be denoted as \\(.^{[... \\times ... \\times \\dots]}\\) where \\(\\times\\) symbol separates across different index sets.\n\nThus our operation becomes:\n\\[\\begin{align}\nB_{s_2 s_1} = attention_{s_1}(Q_{s_2 s_1}, K_{s_3 s_1}, V_{s_3 s_1}) \\\\\nB^{[M \\times d]} = attention_{d}(Q^{[M \\times d]}, K^{[N \\times d]}, V^{[N \\times d]})\n\\end{align}\\]\n\n\\[\\begin{align}\nS^{[M \\times N]} = Q @ K^T = Q *_{(M \\times d, N \\times d, M \\times N)} K \\in \\mathbb{R}^{[M \\times N]} \\\\\nS_{rmax}^{M} = \\max_{N} S^{[M \\times N]} \\in \\mathbb{R}^{M} \\\\\nS_{rm}^{[m, n]} = S^{[m, n]} - S_{rmax}^{m} \\forall [m, n] \\in [M \\times N]  \\\\\nP^{[M \\times N]} = softmax_{N}(S_{rm}^{[M \\times N]}) \\\\\nO^{[M \\times d]} = P @ V = P *_{(M \\times N, N \\times d, M \\times d)} V \\in \\mathbb{R}^{[M \\times d]}\n\\end{align}\\]\n\nNote: Notation abuse -&gt; \\(O \\iff B\\)\n\n\nSimplifying Forward Pass\n\\[\\begin{align}\nP^{[m, n]} = \\frac{\\exp(S^{[m, n]} - \\max_{n}(S^{[m, n]}))}{\\sum_n \\exp(S^{[m, n]} - \\max_{n}(S^{[m, n]}))} = \\frac{\\exp(S^{[m, n]})}{\\sum_n \\exp(S^{[m, n]})}\n\\end{align}\\]\n\nNote: The independence over M in softmax the only aggregation is required over N dimension\n\n\nComputation in chunk\nHere we will first look at what is reuired to generate the output for a single query i.e. \\(O^{[m, d]}\\)\n\\[\\begin{align}\nO^{[m, d]} = P_m *_{(N, N \\times d, d)} V = \\sum_n P^{[m, n]} \\cdot V^{[n, d]} \\\\\nP^{[m, n]} = softmax_{n}(S^{[m, n]}) = \\frac{\\exp(S^{[m, n]})}{\\sum_n \\exp(S^{[m, n]})} \\\\\nO^{[m, d]} = \\sum_n P^{[m, n]} \\cdot V^{[n, d]} = \\sum_n \\frac{\\exp(S^{[m, n]})}{\\sum_n \\exp(S^{[m, n]})} \\cdot V^{[n, d]} \\\\\n= \\frac{1}{\\sum_n \\exp(S^{[m, n]})}\\sum_n \\exp(S^{[m, n]}) \\cdot V^{[n, d]}\n\\end{align}\\]\nWe want to process \\(O^{[m, d]} = \\sum_N \\dots\\) over \\(n\\) sequentially to avoid whole sequence loading.\nfor the sequence just processed till \\(n = j\\) we can write: \\[\\begin{align}\nO^{[m, d]}_j = \\frac{1}{\\sum_{n\\in[0 \\dots j]} \\exp(S^{[m, n]})}\\sum_{n\\in[0 \\dots j]} \\exp(S^{[m, n]}) \\cdot V^{[n, d]} = \\frac{1}{l_j} u_j\n\\end{align}\\]\nLet’s say we proceed by a single setp \\(n = j+1\\): \\[\\begin{align}\nO^{[m, d]}_{j+1} = \\frac{1}{\\sum_{n\\in[0 \\dots j, j+1]} \\exp(S^{[m, n]})}\\sum_{n\\in[0 \\dots j, j+1]} \\exp(S^{[m, n]}) \\cdot V^{[n, d]}\\\\\n= \\frac{1}{l_j + \\exp(S^{[m, n=j+1]})} (u_j + \\exp(S^{[m, n=j+1]}) \\cdot V^{[n=j+1, d]}) \\\\\n= \\frac{O^{[m, d]}_{j} * l_j}{l_{j+1}} + \\frac{\\exp(S^{[m, n=j+1]}) \\cdot V^{[n=j+1, d]}}{l_{j+1}}\n\\end{align}\\]\nThus we can compute the output simply by iterating over the \\(N\\) dimension for \\(O^{m, d}\\) the final expression\n\\[\\begin{align}\nO^{[m, d]}_{0} =  \\frac{\\exp(S^{[m, n=0]}) \\cdot V^{[n=0, d]}}{l_{0}}\\\\\nO^{[m, d]}_{j+1} = \\frac{O^{[m, d]}_{j} * l_j}{l_{j+1}} + \\frac{\\exp(S^{[m, n=j+1]}) \\cdot V^{[n=j+1, d]}}{l_{j+1}} \\\\\nl_0 = exp(S^{[m, n=0]}) \\\\\nl_{j+1} = l_j + exp(S^{[m, n=j+1]})\n\\end{align}\\]\n\n\nWTF: \\(\\exp\\) can explode coz of high multiplication values\nmax operation is used for numerical stability of the softmax especially keeping exp from exploding.\nHere next we will try to encorporate this stabilization technique in the above derived framework.\n\\[\\begin{align}\nm_0 = S^{[m, n=0]} \\\\\nm_{j+1} = \\max(m_j, S^{[m, n=j+1]}) \\\\\nl_0 = \\exp(S^{[m, n=0]} - m_0) \\\\\nl_{j+1} = \\sum_{n\\in[0 \\dots j, j+1]}\\exp(S^{[m, n]} - m_{j+1}) \\\\\nl_{j+1} = \\frac{exp(-m_{j})}{\\exp(m_{j+1} - m_{j})}\\sum_{n\\in[0 \\dots j]}\\exp(S^{[m, n]}) + \\exp(S^{[m, n=j+1]} - m_{j+1}) \\\\\nl_{j+1} = l_{j}\\exp(m_{j} - m_{j+1}) + \\exp(S^{[m, n=j+1]} - m_{j+1}) \\\\\nO^{[m, d]}_{0} = \\frac{\\exp(S^{[m, n=0]} - m_{0}) \\cdot V^{[n=0, d]}}{l_{0}} \\\\\nO^{[m, d]}_{j+1} = \\frac{O^{[m, d]}_{j} * l_j}{l_{j+1}} + \\frac{\\exp(S^{[m, n=j+1]} - m_{j+1}) \\cdot V^{[n=j+1, d]}}{l_{j+1}} \\\\\n\\end{align}\\]"
  }
]