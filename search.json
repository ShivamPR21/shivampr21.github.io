[
  {
    "objectID": "posts/flash-bwd-30-3-2025-kernelized/index.html",
    "href": "posts/flash-bwd-30-3-2025-kernelized/index.html",
    "title": "FlashAttention Kernel: Backward Pass (MATH)",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{s_2 s_1}\\) where \\(s_2\\) and \\(s_1\\) are the index set, for example in a 5D tensor \\(A \\in \\mathbb{R}^{ijklm}\\) a possible index set could be \\(s_2 = \\{ i, j \\}\\) and \\(s_1 = \\{k, l, m \\}\\).\n\\(B_{s_2 s_1} = flash_{s_1}(Q_{s_2 s_1}, K_{s_3 s_1}, V_{s_3 s_1})\\) is a flash attention operation over s1 index set of tensor set \\(&lt;Q, K, V&gt;\\), and the resulting index set is the index set of \\(Q\\) s.t. \\(B \\in \\mathbb{R}^{s_2 s_1}\\)"
  },
  {
    "objectID": "posts/flash-bwd-30-3-2025-kernelized/index.html#preliminary",
    "href": "posts/flash-bwd-30-3-2025-kernelized/index.html#preliminary",
    "title": "FlashAttention Kernel: Backward Pass (MATH)",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{s_2 s_1}\\) where \\(s_2\\) and \\(s_1\\) are the index set, for example in a 5D tensor \\(A \\in \\mathbb{R}^{ijklm}\\) a possible index set could be \\(s_2 = \\{ i, j \\}\\) and \\(s_1 = \\{k, l, m \\}\\).\n\\(B_{s_2 s_1} = flash_{s_1}(Q_{s_2 s_1}, K_{s_3 s_1}, V_{s_3 s_1})\\) is a flash attention operation over s1 index set of tensor set \\(&lt;Q, K, V&gt;\\), and the resulting index set is the index set of \\(Q\\) s.t. \\(B \\in \\mathbb{R}^{s_2 s_1}\\)"
  },
  {
    "objectID": "posts/flash-bwd-30-3-2025-kernelized/index.html#attention-operation",
    "href": "posts/flash-bwd-30-3-2025-kernelized/index.html#attention-operation",
    "title": "FlashAttention Kernel: Backward Pass (MATH)",
    "section": "Attention Operation",
    "text": "Attention Operation\nFor this exercise we will simplify the target index sets to match the most common setup, i.e. \\(Q_{s_2 s_1} \\in \\mathbb{R}^{[M \\times d]}\\), \\(K_{s_3 s_1} \\in \\mathbb{R}^{[N \\times d]}\\), \\(V_{s_3 s_1} \\in \\mathbb{R}^{[N \\times d]}\\)\n\nNote: In this doc, whenever we have to denote the exact dimensions instead of index set it will be denoted as \\(.^{[... \\times ... \\times \\dots]}\\) where \\(\\times\\) symbol separates across different index sets.\n\nThus our operation becomes:\n\\[\\begin{align}\nB_{s_2 s_1} = attention_{s_1}(Q_{s_2 s_1}, K_{s_3 s_1}, V_{s_3 s_1}) \\\\\nB^{[M \\times d]} = attention_{d}(Q^{[M \\times d]}, K^{[N \\times d]}, V^{[N \\times d]})\n\\end{align}\\]\n\n\\[\\begin{align}\nS^{[M \\times N]} = Q @ K^T = Q *_{(M \\times d, N \\times d, M \\times N)} K \\in \\mathbb{R}^{[M \\times N]} \\\\\nS_{rmax}^{M} = \\max_{N} S^{[M \\times N]} \\in \\mathbb{R}^{M} \\\\\nS_{rm}^{[m, n]} = S^{[m, n]} - S_{rmax}^{m} \\forall [m, n] \\in [M \\times N]  \\\\\nP^{[M \\times N]} = softmax_{N}(S_{rm}^{[M \\times N]}) \\\\\nO^{[M \\times d]} = P @ V = P *_{(M \\times N, N \\times d, M \\times d)} V \\in \\mathbb{R}^{[M \\times d]}\n\\end{align}\\]\n\nNote: Notation abuse -&gt; \\(O \\iff B\\)\n\nFor detailed Forward pass derivation please refer to my previous blog: FlashAttention Kernel: Forward Pass (MATH)"
  },
  {
    "objectID": "posts/flash-bwd-30-3-2025-kernelized/index.html#backward-mode-autodiff-pass",
    "href": "posts/flash-bwd-30-3-2025-kernelized/index.html#backward-mode-autodiff-pass",
    "title": "FlashAttention Kernel: Backward Pass (MATH)",
    "section": "Backward (mode autodiff) Pass:",
    "text": "Backward (mode autodiff) Pass:\n\\[\\begin{align}\nB_{s_2 s_1} \\iff B^{[M \\times d]} = attention_{d}(Q^{[M \\times d]}, K^{[N \\times d]}, V^{[N \\times d]})\n\\end{align}\\]\nFor a given loss value \\(O_{s_3 = \\phi}\\) and known \\(dB_{\\phi s_2 s_1} = \\frac{dB_{s_2 s_1}}{dO_{s_3 = \\phi}}\\) We need to find out \\(dQ_{\\phi s_2 s_1}\\), \\(dK_{\\phi s_3 s_1}\\), and \\(dV_{\\phi s_3 s_1}\\).\nHere we will directly differentiate the core attention operation without adjusting for numerical stability of exponent (we did so in forward pass to just make computation stable). Here we will first derive the core backward operations and then change it to computation, followed by mitigating any source of numerical instability.\n\n\\(dV_{\\phi s_3 s_1}\\):\nConsider following op: \\(B_{s_2 s_1} = \\sum_{s_3} P_{s_2 s_3} \\cdot V_{s_3 s_1} = P^{[M \\times N]} @ V^{[N \\times d]} \\in \\mathbb{R}^{[M \\times d]}_{s_2 s_1}\\)\n\\[\\begin{align}\ndV_{\\phi s'_3 s'_1} = \\frac{\\partial O_{\\phi}}{\\partial V_{s'_3 s'_1}} = \\sum_{s_2 s_1} \\frac{\\partial O_{\\phi}}{\\partial B_{s_2 s_1}} \\frac{\\partial B_{s_2 s_1}}{\\partial V_{s'_3 s'_1}} = \\sum_{s_2 s_1} dB_{s_2 s_1} \\frac{\\partial B_{s_2 s_1}}{\\partial V_{s'_3 s'_1}} \\\\\n\\frac{\\partial B_{s_2 s_1}}{\\partial V_{s'_3 s'_1}} = \\sum_{s_3} P_{s_2 s_3} \\mathbb{1}_{(s_3 s_1) = (s'_3 s'_1)} = P_{s_2 s'_3} \\mathbb{1}_{s_1 = s'_1} \\\\\n\\sum_{s_2 s_1} dB_{s_2 s_1} \\frac{\\partial B_{s_2 s_1}}{\\partial V_{s'_3 s'_1}} = \\sum_{s_2 s_1} dB_{s_2 s_1} P_{s_2 s'_3} \\mathbb{1}_{s_1 = s'_1} = \\sum_{s_2} dB_{s_2 s'_1} P_{s_2 s'_3} \\\\\ndV_{\\phi s'_3 s'_1} = \\sum_{s_2 s_1} dB_{s_2 s_1} \\frac{\\partial B_{s_2 s_1}}{\\partial V_{s'_3 s'_1}} = \\sum_{s_2} dB_{s_2 s'_1} P_{s_2 s'_3} = dB^T \\cdot P\n\\end{align}\\]\n\nNote: This also provides a crucial propetry of tensor differentiation i.e. for tensor product operation \\(C_{s_2 s_3} = \\sum_{s_1} A_{s_2 s_1} B_{s_1 s_3} = A \\cdot B\\) then for a given \\(dC_{s_o s_2 s_3}\\) the derivative \\(dA_{s_o s'_2 s'_1} = \\sum_{s_3} dC_{s_o s'_2 s_3} B_{s'_1 s_3}\\) for a simple matmul i.e. \\(s_o = \\phi, s_3 \\in \\mathbb{R}, s_2 \\in \\mathbb{R}, s_1 \\in \\mathbb{R}\\) this operation shrinks to simply \\(dA = dC \\cdot B^T\\). Similarly for \\(dB_{s_o s'_1 s'_3} = \\sum_{s_2} dC_{s_o s_2 s'_3} A_{s_2 s'_1}\\) for a simple matrix multiplication this would reduce to \\(dB = dC^T \\cdot A\\)\n\n\n\n\\(dP_{\\phi s_2 s_3}\\):\nFrom the formula derived previously \\(dP_{\\phi s'_2 s'_3} = \\sum_{s_1} dB_{s'_2 s_1} V_{s'_3 s_1} = dB \\cdot V^T\\)\n\n\n\\(dQK^T_{\\phi s_2 s_3}\\)\nHere we have encountered softmax operation as \\(P_{s_2 s_3} = softmax_{s_3}(S_{s_2 s_3} = QK^T_{s_2 s_3})\\) from the softmax blog we can\n\nDirect Operation:\n\\[\\begin{equation}\nB_{s_2 s_1} = softmax_{s_1}(A_{s_2 s_1})\n\\end{equation}\\]\nHere \\(O_{s_3}\\) is the final loss value for which we need to extract the derivatives.\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}} = B_{s'_2 s'_1} \\left[dB_{s_3 s'_2 s'_1} - \\sum_{s_1} dB_{s_3 s'_2 s_1} B_{s'_2 s_1}\\right]\n\\end{align}\\]\n\n\nFrom the formula, we can say:\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial QK^T_{s'_2 s'_3}} = P_{s'_2 s'_3} \\left[dP_{\\phi s'_2 s'_3} -  \\sum_{s_3} dP_{\\phi s'_2 s_3} P_{s'_2 s_3}\\right]\n\\end{align}\\]\nfor a simple matmul:\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial S_{i'j'}} = P_{i'j'} \\left[dP_{i'j'} - dP_{i':}^T \\circ P_{i':} \\right] = P \\times \\left[dP - BMM(dP_{i'1j, P_{i'j1}}) \\right]\n\\end{align}\\]\n\n\n\n\\(dQ_{\\phi s_2 s_1}\\) & \\(dK_{\\phi s_2 s_1}\\):\n\\(S = QK^T\\) and we know \\(dS\\) thus we can directly write the derivatives of both \\(Q\\) and \\(K\\).\n\\[\\begin{align}\ndQ = dS \\cdot K \\\\\ndK = dS^T \\cdot Q\n\\end{align}\\]\n\n\nFinal Backward Pass Equations:\n\\[\\begin{align}\ndB \\in \\mathbb{R}^{[M \\times D]}, \\{Q, dQ\\} \\in \\mathbb{R}^{[M \\times D]}, \\\\ \\{K, dK\\} \\in \\mathbb{R}^{[N \\times D]}, \\{V, dV\\} \\in \\mathbb{R}^{[N \\times D]}\\\\\n\\end{align}\\]\n\n\\[\\begin{align}\ndV = dB^T \\cdot P \\\\\ndP = dB \\cdot V^T \\\\\ndS_{i'j'} = P_{i'j'} \\left[dP_{i'j'} - dP_{i':}^T \\circ P_{i':} \\right] \\\\\ndQ = dS \\cdot K \\\\\ndK = dS^T \\cdot Q\n\\end{align}\\]\n\nExpansion in dimensions:\n\\[\\begin{align}\nS_{ij} = \\sum_d q_{i d} k_{j d} \\\\\ndV_{j d} = dB^T \\cdot P = \\sum_i dB_{i d} P_{i j} = \\sum_i dB_{i d} \\frac{\\exp(S_{i j})}{L_i} \\\\\ndP_{i j} = dB \\cdot V^T = \\sum_d dB_{i d} V_{j d} \\\\\ndS_{i j} = P_{i j} \\left[dP_{i j} - \\sum_j dP_{i j} P_{i j} \\right] \\\\\ndQ_{i d} = dS \\cdot K = \\sum_{j} dS_{i j} K_{j d}\\\\\ndK_{j d} = dS^T \\cdot Q = \\sum_{i} dS_{i j} Q_{i d}\n\\end{align}\\]\n\n\nAbstract away D dimension:\n\nIn a future blog we will see that this is natural to do computation along D (embedding) dimension as all of the computations are independent of each other in this dimension.\n\n\\[\\begin{align}\nS_{ij} = q_i \\circ k_j \\\\\ndV_j = \\sum_i dB_{i d} \\frac{\\exp(q_i \\circ k_j)}{L_i} \\\\\ndP_{i j} = dB_i \\circ V_j \\\\\ndS_{i j} = P_{i j} \\left[dP_{i j} - \\sum_j dP_{i j} P_{i j} \\right] \\\\\ndQ_i = dS_i \\circ K_j \\\\\ndK_j = dS^T_j \\circ Q_i \\\\\n\\end{align}\\]\n\n\\[\\begin{align}\n\\sum_j dP_{i j} P_{i j} = \\sum_j \\big(\\sum_d dB_{i d} V_{j d}\\big) P_{i j} = \\sum_j \\sum_d dB_{i d} V_{j d} P_{i j} \\\\\n= \\sum_d \\sum_j dB_{i d} V_{j d} P_{i j} = \\sum_d dB_{i d} \\sum_j V_{j d} P_{i j} = \\sum_d dB_{i d} B_{i d}\n\\end{align}\\]\n\\[\\begin{align}\nS_{ij} = q_i \\circ k_j \\\\\ndV_j = \\sum_i dB_{i d} \\frac{\\exp(q_i \\circ k_j)}{L_i} \\\\\ndP_{i j} = dB_i \\circ V_j \\\\\ndS_{i j} = P_{i j} \\left[dP_{i j} - dB_i \\circ B_i \\right] \\\\\ndQ_i = dS_i \\circ K_j \\\\\ndK_j = dS^T_j \\circ Q_i \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "posts/softmax-30-3-2025-kernelized/index.html",
    "href": "posts/softmax-30-3-2025-kernelized/index.html",
    "title": "SoftMax Kernel: Forward and Backward Pass (MATH)",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{s_2 s_1}\\) where \\(s_2\\) and \\(s_1\\) are the index set, for example in a 5D tensor \\(A \\in \\mathbb{R}^{ijklm}\\) a possible index set could be \\(s_2 = \\{ i, j \\}\\) and \\(s_1 = \\{k, l, m \\}\\).\n\\(B_{s_2 s_1} = softmax_{s_1}(A_{s_2 s_1})\\) is a softmax operation over s1 index set of A, and the resulting index set still remains same as A s.t. \\(B \\in \\mathbb{R}^{s2s1}\\)"
  },
  {
    "objectID": "posts/softmax-30-3-2025-kernelized/index.html#softmax-operation",
    "href": "posts/softmax-30-3-2025-kernelized/index.html#softmax-operation",
    "title": "SoftMax Kernel: Forward and Backward Pass (MATH)",
    "section": "Softmax Operation",
    "text": "Softmax Operation\n\\[\\begin{equation}\nB_{s_2 s_1} = softmax_{s_1}(A_{s_2 s_1})\n\\end{equation}\\]\n\nIntermediate Result: #1\n\\[\\begin{equation}\nI^1_{s_2 s_1} = \\exp{(A_{s_2 s_1})}\n\\end{equation}\\]\n\n\nIntermediate Result: #2\n\\[\\begin{equation}\nI^2_{s_2} = \\sum_{s1}I^1_{s_2 s_1}\n\\end{equation}\\]\n\n\nSoftmax:\n\\[\\begin{equation}\nB_{s_2 s_1} = \\frac{I^1_{s_2 s_1}}{I^2_{s_2}} = \\frac{\\exp{(A_{s_2 s_1})}}{\\sum_{s1}\\exp{(A_{s_2 s_1})}}\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/softmax-30-3-2025-kernelized/index.html#backward-pass",
    "href": "posts/softmax-30-3-2025-kernelized/index.html#backward-pass",
    "title": "SoftMax Kernel: Forward and Backward Pass (MATH)",
    "section": "Backward Pass:",
    "text": "Backward Pass:\nHere we will just look at the backward pass of the softmax kernel alone, as it will help us understand a much wider concept of having a multidimensional loss function instead of a scalar loss.\nFortunately this also simplifies the problem for us as we won’t have to account for any pullbacks for the output \\(B_{s_2 s_1}\\) itself.\n\\[\\begin{equation}\n\\frac{\\partial B_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} \\in \\mathbb{R}^{s_2 s_1 s'_2 s'_1}\n\\end{equation}\\]\n\nDerivation:\n\\[\\begin{align}\n\\frac{\\partial B_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} = \\frac{1}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\frac{\\partial \\exp{(A_{s_2 s_1})}}{\\partial A_{s'_2 s'_1}} + \\exp(A_{s_2 s_1})\\frac{\\partial \\sum_{s_1} 1/exp(A_{s_2 s_1})}{\\partial A_{s'_2 s'_1}}\n\\end{align}\\]\n\n\\[\\begin{align}\n\\frac{1}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\frac{\\partial \\exp{(A_{s_2 s_1})}}{\\partial A_{s'_2 s'_1}} = \\frac{\\exp(A_{s_2 s_1})}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\frac{A_{s_2 s_1}}{A_{s'_2 s'_1}} = \\frac{\\exp(A_{s_2 s_1})}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)}\n\\end{align}\\]\n\n\\[\\begin{equation}\n\\exp(A_{s_2 s_1})\\frac{\\partial \\sum_{s_1} 1/exp(A_{s_2 s_1})}{\\partial A_{s'_2 s'_1}} = -\\frac{\\exp(A_{s_2 s_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\frac{\\partial \\sum_{s_1} \\exp(A_{s_2 s_1})}{\\partial A_{s'_2 s'_1}}\n\\end{equation}\\]\n\\[\\begin{align}\n\\frac{\\partial \\sum_{s_1} \\exp(A_{s_2 s_1})}{\\partial A_{s'_2 s'_1}} = \\sum_{s_1} \\frac{\\partial \\exp(A_{s_2 s_1})}{\\partial A_{s'_2 s'_1}} = \\sum_{s_1} \\exp(A_{s_2 s_1}) \\frac{\\partial A_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} \\\\ = \\sum_{s_1} \\exp(A_{s_2 s_1}) \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} = \\exp(A_{s_2 s'_1}) \\mathbb{1}_{s_2 = s'_2}\n\\end{align}\\]\n\n\nFinal Derivative Simplification:\n\n\\[\\begin{align}\n\\frac{\\partial B_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} = \\frac{\\exp(A_{s_2 s_1})}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} - \\frac{\\exp(A_{s_2 s_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\exp(A_{s_2 s'_1}) \\mathbb{1}_{s_2 = s'_2} \\\\ = \\frac{\\exp(A_{s_2 s_1})}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} - \\frac{\\exp(A_{s_2 s_1}) \\exp(A_{s_2 s'_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\mathbb{1}_{s_2 = s'_2} \\\\\n= B_{s_2 s_1} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} - B_{s_2 s_1}B_{s_2 s'_1} \\mathbb{1}_{s_2 = s'_2}\n\\end{align}\\]\n\n\n\\[\\begin{align}\n\\frac{\\partial B_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} = B_{s_2 s_1} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} - B_{s_2 s_1}B_{s_2 s'_1} \\mathbb{1}_{s_2 = s'_2}\n\\end{align}\\]\n\n\n\nFull reduction: With Loss Drivative\nLet’s assume the output \\(B_{s_2 s_1}\\) being used by some frisky function to generate loss \\(O_{s_3}\\) and somehow we have the pullback of \\(B\\) w.r.t. \\(O\\) as \\(dB_{s_3 s_2 s_1} = \\frac{\\partial O_{s_3}}{\\partial B_{s_2 s_1}}\\), and now we are interested in find out what’s the pullback of \\(A_{s'_2 s'_1}\\) w.r.t. \\(O_{s_3}\\) i.e. \\(\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}}\\).\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}} = \\sum_{s_2 s_1} \\frac{\\partial O_{s_3}}{\\partial B_{s_2 s_1}}\\frac{\\partial B_{s_2 s_1}}{\\partial A_{s'_2 s'_1}} \\\\\n= \\sum_{s_2 s_1} dB_{s_3 s_2 s_1} \\left[ \\frac{\\exp(A_{s_2 s_1})}{\\sum_{s_1}\\exp{(A_{s_2 s_1})}} \\mathbb{1}_{(s_2 s_1) = (s'_2 s'_1)} - \\frac{\\exp(A_{s_2 s_1}) \\exp(A_{s_2 s'_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\mathbb{1}_{s_2 = s'_2} \\right] \\\\\n= dB_{s_3 s'_2 s'_1} \\frac{\\exp(A_{s'_2 s'_1})}{\\sum_{s_1}\\exp{(A_{s'_2 s_1})}} - \\sum_{s_2 s_1} dB_{s_3 s_2 s_1} \\frac{\\exp(A_{s_2 s_1}) \\exp(A_{s_2 s'_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\mathbb{1}_{s_2 = s'_2}\n\\end{align}\\]\n\n\\[\\begin{align}\n\\sum_{s_2 s_1} dB_{s_3 s_2 s_1} \\frac{\\exp(A_{s_2 s_1}) \\exp(A_{s_2 s'_1})}{[\\sum_{s_1} exp(A_{s_2 s_1})]^2} \\mathbb{1}_{s_2 = s'_2} = \\sum_{s_1} dB_{s_3 s'_2 s_1} \\frac{\\exp(A_{s'_2 s_1}) \\exp(A_{s'_2 s'_1})}{[\\sum_{s_1} exp(A_{s'_2 s_1})]^2} \\\\\n= \\frac{\\exp(A_{s'_2 s'_1})}{[\\sum_{s_1} exp(A_{s'_2 s_1})]^2} \\sum_{s_1} [dB_{s_3 s'_2 s_1} \\exp(A_{s'_2 s_1})]\n\\end{align}\\]\n\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}}\n= dB_{s_3 s'_2 s'_1} \\frac{\\exp(A_{s'_2 s'_1})}{\\sum_{s_1}\\exp{(A_{s'_2 s_1})}} - \\frac{\\exp(A_{s'_2 s'_1})}{[\\sum_{s_1} exp(A_{s'_2 s_1})]^2} \\sum_{s_1} [dB_{s_3 s'_2 s_1} \\exp(A_{s'_2 s_1})] \\\\\n= dB_{s_3 s'_2 s'_1} B_{s'_2 s'_1} - B_{s'_2 s'_1}\\frac{1}{\\sum_{s_1} exp(A_{s'_2 s_1})} \\sum_{s_1} [dB_{s_3 s'_2 s_1} \\exp(A_{s'_2 s_1})] \\\\\n= dB_{s_3 s'_2 s'_1} B_{s'_2 s'_1} - B_{s'_2 s'_1} \\sum_{s_1} [dB_{s_3 s'_2 s_1} B_{s'_2 s_1}] = B_{s'_2 s'_1} [dB_{s_3 s'_2 s'_1} - \\sum_{s_1} [dB_{s_3 s'_2 s_1} B_{s'_2 s_1}]]\n\\end{align}\\]\n\n\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}} = B_{s'_2 s'_1} \\left[dB_{s_3 s'_2 s'_1} - \\sum_{s_1} dB_{s_3 s'_2 s_1} B_{s'_2 s_1}\\right]\n\\end{align}\\]"
  },
  {
    "objectID": "posts/softmax-30-3-2025-kernelized/index.html#application-in-attention-specialize-the-expressions",
    "href": "posts/softmax-30-3-2025-kernelized/index.html#application-in-attention-specialize-the-expressions",
    "title": "SoftMax Kernel: Forward and Backward Pass (MATH)",
    "section": "Application in Attention: Specialize the Expressions",
    "text": "Application in Attention: Specialize the Expressions\n\nSetup:\n\\[\\begin{align}\nP_{ij} = softmax(S_{ij}) \\\\\nS_{ij} \\in \\mathbb{R}^{M \\times N} \\\\\nP_{ij} \\in \\mathbb{R}^{M \\times N} \\\\\nO_{\\phi} \\in \\mathbb{R} \\\\\ndP_{\\phi ij} = \\frac{\\partial O_{\\phi}}{\\partial P_{ij}} \\in \\mathbb{R}^{\\phi \\times M \\times N} \\implies \\text{Known}\n\\end{align}\\]\n\n\nDifferentiation:\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial S_{i'j'}} = \\sum_{ij} \\frac{\\partial O_{\\phi}}{\\partial P_{ij}}\\frac{\\partial P_{ij}}{\\partial S_{i'j'}} = P_{i'j'} \\left[dP_{\\phi i'j'} - \\sum_{j} dP_{\\phi i'j} P_{i'j} \\right] \\\\\n= P_{i'j'} \\left[dP_{i'j'} - \\sum_{j} dP_{i'j} P_{i'j} \\right] = P_{i'j'} \\left[dP_{i'j'} - dP_{i':}^T \\circ P_{i':} \\right] \\\\\n= P \\times \\left[dP - BMM(dP_{i'1j, P_{i'j1}}) \\right]\n\\end{align}\\]\n\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial S_{i'j'}} = P_{i'j'} \\left[dP_{i'j'} - dP_{i':}^T \\circ P_{i':} \\right]\n\\end{align}\\]"
  },
  {
    "objectID": "kernelized.html",
    "href": "kernelized.html",
    "title": "KERNELIZED",
    "section": "",
    "text": "FlashAttention Kernel: Forward Pass (Parallelism)\n\n\n\n\n\n\nFlashAttention\n\n\nTransformers\n\n\nAttention\n\n\nCompute\n\n\nAutograd\n\n\nParallelism\n\n\nCUDA\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nShivam Pandey\n\n\n\n\n\n\n\n\n\n\n\n\nMax Kernel: Forward and Backward Pass (MATH)\n\n\n\n\n\n\nAI\n\n\nCompute\n\n\nAutograd\n\n\nMATH\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nShivam Pandey\n\n\n\n\n\n\n\n\n\n\n\n\nSoftMax Kernel: Forward and Backward Pass (MATH)\n\n\n\n\n\n\nAI\n\n\nCompute\n\n\nAutograd\n\n\nMATH\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nShivam Pandey\n\n\n\n\n\n\n\n\n\n\n\n\nFlashAttention Kernel: Forward Pass (MATH)\n\n\n\n\n\n\nFlashAttention\n\n\nTransformers\n\n\nAttention\n\n\nCompute\n\n\nAutograd\n\n\nMATH\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nShivam Pandey\n\n\n\n\n\n\n\n\n\n\n\n\nFlashAttention Kernel: Backward Pass (MATH)\n\n\n\n\n\n\nFlashAttention\n\n\nTransformers\n\n\nAttention\n\n\nCompute\n\n\nAutograd\n\n\nMATH\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nShivam Pandey\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shivam Pandey",
    "section": "",
    "text": "Welcome to my site! I’m Shivam Pandey, a tech explorer dedicated to advancing AI’s explainability and its ability to generalize across tasks with minimal data.\n\n\nScaling computes, not comprehension.\n\nEven massive compute and oceans of data only deliver brute-force wins—they scale performance, not true insight. These systems remain data inefficient and fail to generalize, missing the mark on real intelligence.\n\nCurrently I’m working on Quantifying the Intelligence-Compute Decoupling in Modern AI Systems.\n\n\n\nKERNELIZED is a blog series around exploring the territory of Modern Compute for AI from both algorithmic and infrastructure point of view. Some of the initial goal for this blog series includes:\n\nEasy scaling of new AI algorithms to modern compute hardware (e.g. GPU, TPU, WSE etc.)\nExplore and not Rush: The blogs should be exploratory i.e. instead of providing best practices, it enables the way to get there, while not hindering the potential of the information to be applied to other algorithms.\nThe blogs are \\(\\mu\\)-BLOGS: Read time should not exceed more than 10 minutes. This particular requirement comes from my personal experience that longer blogs/write-ups hinder information sharing no matter how good it is written.\nBe as raw as possible, no hiding of details in the name of accessibility. Specific writing style is too accessible nowadays, just ask ChatGPT, Claude, and DeepSeek-R1 given the link to blog :wink:\n\n\n\n\nAnother blog series will soon be released targeting AI Explainability and AI Security, where the target will be breaking down AI models to their core logic and using that information to assess potential security issues.\nKeep Checking this page for further updates as the Code analysis and implementation are dropping next in KERNELIZED series, and other upcoming projects.\n\n\n Shivam Pandey a.k.a. ShivamPR21"
  },
  {
    "objectID": "index.html#research-orientation-wip",
    "href": "index.html#research-orientation-wip",
    "title": "Shivam Pandey",
    "section": "",
    "text": "Scaling computes, not comprehension.\n\nEven massive compute and oceans of data only deliver brute-force wins—they scale performance, not true insight. These systems remain data inefficient and fail to generalize, missing the mark on real intelligence.\n\nCurrently I’m working on Quantifying the Intelligence-Compute Decoupling in Modern AI Systems."
  },
  {
    "objectID": "index.html#kernelized",
    "href": "index.html#kernelized",
    "title": "Shivam Pandey",
    "section": "",
    "text": "KERNELIZED is a blog series around exploring the territory of Modern Compute for AI from both algorithmic and infrastructure point of view. Some of the initial goal for this blog series includes:\n\nEasy scaling of new AI algorithms to modern compute hardware (e.g. GPU, TPU, WSE etc.)\nExplore and not Rush: The blogs should be exploratory i.e. instead of providing best practices, it enables the way to get there, while not hindering the potential of the information to be applied to other algorithms.\nThe blogs are \\(\\mu\\)-BLOGS: Read time should not exceed more than 10 minutes. This particular requirement comes from my personal experience that longer blogs/write-ups hinder information sharing no matter how good it is written.\nBe as raw as possible, no hiding of details in the name of accessibility. Specific writing style is too accessible nowadays, just ask ChatGPT, Claude, and DeepSeek-R1 given the link to blog :wink:"
  },
  {
    "objectID": "index.html#next",
    "href": "index.html#next",
    "title": "Shivam Pandey",
    "section": "",
    "text": "Another blog series will soon be released targeting AI Explainability and AI Security, where the target will be breaking down AI models to their core logic and using that information to assess potential security issues.\nKeep Checking this page for further updates as the Code analysis and implementation are dropping next in KERNELIZED series, and other upcoming projects."
  },
  {
    "objectID": "posts/max-30-3-2025-kernelized/index.html",
    "href": "posts/max-30-3-2025-kernelized/index.html",
    "title": "Max Kernel: Forward and Backward Pass (MATH)",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{s_2 s_1}\\) where \\(s_2\\) and \\(s_1\\) are the index set, for example in a 5D tensor \\(A \\in \\mathbb{R}^{ijklm}\\) a possible index set could be \\(s_2 = \\{ i, j \\}\\) and \\(s_1 = \\{k, l, m \\}\\).\n\\(B_{s_2} = max_{s_1}(A_{s_2 s_1})\\) is a max operation over s1 index set of A, and the resulting index set is the index set of \\(A\\) reduced over \\(s_1\\) s.t. \\(B \\in \\mathbb{R}^{s_2}\\)"
  },
  {
    "objectID": "posts/max-30-3-2025-kernelized/index.html#max-reduction-operation",
    "href": "posts/max-30-3-2025-kernelized/index.html#max-reduction-operation",
    "title": "Max Kernel: Forward and Backward Pass (MATH)",
    "section": "Max (Reduction) operation",
    "text": "Max (Reduction) operation\n\\[\\begin{align}\nB_{s_2} = \\max_{s_1}(A_{s_2 s_1}) = A_{s_2 s^m_1} \\big|_{s^m_1 = argmax_{s_1}(A_{s_2 s_1})}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/max-30-3-2025-kernelized/index.html#backward-pass",
    "href": "posts/max-30-3-2025-kernelized/index.html#backward-pass",
    "title": "Max Kernel: Forward and Backward Pass (MATH)",
    "section": "Backward Pass:",
    "text": "Backward Pass:\nHere we will deduce the pullback of \\(A\\) under the \\(\\max\\) operation, w.r.t. \\(B\\).\n\\[\\begin{align}\n\\frac{\\partial B_{s_2}}{\\partial A_{s'_2 s'_1}} = \\frac{\\partial A_{s_2 s^m_1}}{\\partial A_{s'_2 s'_1}} = \\mathbb{1}_{s_2 = s'_2} \\cdot \\mathbb{1}_{s^m_1 = s'_1}\n\\end{align}\\]\n\nFull reduction: With Loss Drivative\nLet’s assume the output \\(B_{s_2}\\) being used by some frisky function to generate loss \\(O_{s_3}\\) and somehow we have the pullback of \\(B\\) w.r.t. \\(O\\) as \\(dB_{s_3 s_2} = \\frac{\\partial O_{s_3}}{\\partial B_{s_2}}\\), and now we are interested in find out what’s the pullback of \\(A_{s'_2 s'_1}\\) w.r.t. \\(O_{s_3}\\) i.e. \\(\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}}\\).\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}} = \\sum_{s_2} \\frac{\\partial O_{s_3}}{\\partial B_{s_2}} \\frac{\\partial B_{s_2}}{\\partial A_{s'_2 s'_1}} = \\sum_{s_2} dB_{s_3 s_2} \\mathbb{1}_{s_2 = s'_2} \\cdot \\mathbb{1}_{s^m_1 = s'_1} = dB_{s_3 s'_2} \\mathbb{1}_{s^m_1 = s'_1} \\big|_{s^m_1 = argmax_{s_1}(A_{s'_2 s_1})}\n\\end{align}\\]\n\n\\[\\begin{align}\n\\frac{\\partial O_{s_3}}{\\partial A_{s'_2 s'_1}} = dB_{s_3 s'_2} \\mathbb{1}_{s^m_1 = s'_1} \\big|_{s^m_1 = argmax_{s_1}(A_{s'_2 s_1})}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/max-30-3-2025-kernelized/index.html#application-in-attention-specialize-the-expressions",
    "href": "posts/max-30-3-2025-kernelized/index.html#application-in-attention-specialize-the-expressions",
    "title": "Max Kernel: Forward and Backward Pass (MATH)",
    "section": "Application in Attention: Specialize the Expressions",
    "text": "Application in Attention: Specialize the Expressions\n\nSetup:\n\\[\\begin{align}\nS_{ij} = S^p_{ij} - \\max_j(S^p_{ij}) \\\\\nP_{ij} = softmax(S_{ij}) \\\\\nS_{ij} \\in \\mathbb{R}^{M \\times N} \\\\\nP_{ij} \\in \\mathbb{R}^{M \\times N} \\\\\nO_{\\phi} \\in \\mathbb{R} \\\\\ndP_{\\phi ij} = \\frac{\\partial O_{\\phi}}{\\partial P_{ij}} \\in \\mathbb{R}^{\\phi \\times M \\times N} \\implies \\text{Known} \\\\\ndS_{\\phi ij} = \\frac{\\partial O_{\\phi}}{\\partial S_{ij}} \\in \\mathbb{R}^{\\phi \\times M \\times N} =  P_{ij} \\left[dP_{ij} - dP_{i:}^T \\circ P_{i:} \\right]\n\\end{align}\\]\n\n\nDifferentiation\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial S^p_{i'j'}} = \\sum_{ij} \\frac{\\partial O_{\\phi}}{\\partial S_{ij}} \\frac{\\partial S_{ij}}{\\partial S^p_{i'j'}} = \\sum_{ij} dS_{\\phi ij} (\\mathbb{1}_{ij = i'j'} - \\mathbb{1}_{i = i'} \\mathbb{1}_{j^m = j'} \\big|_{argmax_{j}(S^p_{i'j})}) \\\\\n= dS_{i'j'} - \\sum_{j} dS_{i'j} \\mathbb{1}_{j^m = j'} \\big|_{j^m={argmax_{j}(S^p_{i'j})}} = dS_{i'j'} - \\mathbb{1}_{j^m = j'} \\big|_{j^m={argmax_{j}(S^p_{i'j})}} \\sum_{j} dS_{i'j}\n\\end{align}\\]\n\nReplacing \\(dS_{ij}\\):\n\\[\\begin{align}\n\\frac{\\partial O_{\\phi}}{\\partial S^p_{i'j'}} = dS_{i'j'} - \\mathbb{1}_{j^m = j'} \\big|_{j^m={argmax_{j}(S^p_{i'j})}} \\sum_{j} dS_{i'j} \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "posts/flash-30-3-2025-kernelized/index.html",
    "href": "posts/flash-30-3-2025-kernelized/index.html",
    "title": "FlashAttention Kernel: Forward Pass (MATH)",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{s_2 s_1}\\) where \\(s_2\\) and \\(s_1\\) are the index set, for example in a 5D tensor \\(A \\in \\mathbb{R}^{ijklm}\\) a possible index set could be \\(s_2 = \\{ i, j \\}\\) and \\(s_1 = \\{k, l, m \\}\\).\n\\(B_{s_2 s_1} = flash_{s_1}(Q_{s_2 s_1}, K_{s_3 s_1}, V_{s_3 s_1})\\) is a flash attention operation over s1 index set of tensor set \\(&lt;Q, K, V&gt;\\), and the resulting index set is the index set of \\(Q\\) s.t. \\(B \\in \\mathbb{R}^{s_2 s_1}\\)"
  },
  {
    "objectID": "posts/flash-30-3-2025-kernelized/index.html#attention-operation",
    "href": "posts/flash-30-3-2025-kernelized/index.html#attention-operation",
    "title": "FlashAttention Kernel: Forward Pass (MATH)",
    "section": "Attention Operation",
    "text": "Attention Operation\nFor this exercise we will simplify the target index sets to match the most common setup, i.e. \\(Q_{s_2 s_1} \\in \\mathbb{R}^{[M \\times d]}\\), \\(K_{s_3 s_1} \\in \\mathbb{R}^{[N \\times d]}\\), \\(V_{s_3 s_1} \\in \\mathbb{R}^{[N \\times d]}\\)\n\nNote: In this doc, whenever we have to denote the exact dimensions instead of index set it will be denoted as \\(.^{[... \\times ... \\times \\dots]}\\) where \\(\\times\\) symbol separates across different index sets.\n\nThus our operation becomes:\n\\[\\begin{align}\nB_{s_2 s_1} = attention_{s_1}(Q_{s_2 s_1}, K_{s_3 s_1}, V_{s_3 s_1}) \\\\\nB^{[M \\times d]} = attention_{d}(Q^{[M \\times d]}, K^{[N \\times d]}, V^{[N \\times d]})\n\\end{align}\\]\n\n\\[\\begin{align}\nS^{[M \\times N]} = Q @ K^T = Q *_{(M \\times d, N \\times d, M \\times N)} K \\in \\mathbb{R}^{[M \\times N]} \\\\\nS_{rmax}^{M} = \\max_{N} S^{[M \\times N]} \\in \\mathbb{R}^{M} \\\\\nS_{rm}^{[m, n]} = S^{[m, n]} - S_{rmax}^{m} \\forall [m, n] \\in [M \\times N]  \\\\\nP^{[M \\times N]} = softmax_{N}(S_{rm}^{[M \\times N]}) \\\\\nO^{[M \\times d]} = P @ V = P *_{(M \\times N, N \\times d, M \\times d)} V \\in \\mathbb{R}^{[M \\times d]}\n\\end{align}\\]\n\nNote: Notation abuse -&gt; \\(O \\iff B\\)\n\n\nSimplifying Forward Pass\n\\[\\begin{align}\nP^{[m, n]} = \\frac{\\exp(S^{[m, n]} - \\max_{n}(S^{[m, n]}))}{\\sum_n \\exp(S^{[m, n]} - \\max_{n}(S^{[m, n]}))} = \\frac{\\exp(S^{[m, n]})}{\\sum_n \\exp(S^{[m, n]})}\n\\end{align}\\]\n\nNote: The independence over M in softmax the only aggregation is required over N dimension\n\n\nComputation in chunk\nHere we will first look at what is reuired to generate the output for a single query i.e. \\(O^{[m, d]}\\)\n\\[\\begin{align}\nO^{[m, d]} = P_m *_{(N, N \\times d, d)} V = \\sum_n P^{[m, n]} \\cdot V^{[n, d]} \\\\\nP^{[m, n]} = softmax_{n}(S^{[m, n]}) = \\frac{\\exp(S^{[m, n]})}{\\sum_n \\exp(S^{[m, n]})} \\\\\nO^{[m, d]} = \\sum_n P^{[m, n]} \\cdot V^{[n, d]} = \\sum_n \\frac{\\exp(S^{[m, n]})}{\\sum_n \\exp(S^{[m, n]})} \\cdot V^{[n, d]} \\\\\n= \\frac{1}{\\sum_n \\exp(S^{[m, n]})}\\sum_n \\exp(S^{[m, n]}) \\cdot V^{[n, d]}\n\\end{align}\\]\nWe want to process \\(O^{[m, d]} = \\sum_N \\dots\\) over \\(n\\) sequentially to avoid whole sequence loading.\nfor the sequence just processed till \\(n = j\\) we can write: \\[\\begin{align}\nO^{[m, d]}_j = \\frac{1}{\\sum_{n\\in[0 \\dots j]} \\exp(S^{[m, n]})}\\sum_{n\\in[0 \\dots j]} \\exp(S^{[m, n]}) \\cdot V^{[n, d]} = \\frac{1}{l_j} u_j\n\\end{align}\\]\nLet’s say we proceed by a single setp \\(n = j+1\\): \\[\\begin{align}\nO^{[m, d]}_{j+1} = \\frac{1}{\\sum_{n\\in[0 \\dots j, j+1]} \\exp(S^{[m, n]})}\\sum_{n\\in[0 \\dots j, j+1]} \\exp(S^{[m, n]}) \\cdot V^{[n, d]}\\\\\n= \\frac{1}{l_j + \\exp(S^{[m, n=j+1]})} (u_j + \\exp(S^{[m, n=j+1]}) \\cdot V^{[n=j+1, d]}) \\\\\n= \\frac{O^{[m, d]}_{j} * l_j}{l_{j+1}} + \\frac{\\exp(S^{[m, n=j+1]}) \\cdot V^{[n=j+1, d]}}{l_{j+1}}\n\\end{align}\\]\nThus we can compute the output simply by iterating over the \\(N\\) dimension for \\(O^{m, d}\\) the final expression\n\\[\\begin{align}\nO^{[m, d]}_{0} =  \\frac{\\exp(S^{[m, n=0]}) \\cdot V^{[n=0, d]}}{l_{0}}\\\\\nO^{[m, d]}_{j+1} = \\frac{O^{[m, d]}_{j} * l_j}{l_{j+1}} + \\frac{\\exp(S^{[m, n=j+1]}) \\cdot V^{[n=j+1, d]}}{l_{j+1}} \\\\\nl_0 = exp(S^{[m, n=0]}) \\\\\nl_{j+1} = l_j + exp(S^{[m, n=j+1]})\n\\end{align}\\]\n\n\nWTF: \\(\\exp\\) can explode coz of high multiplication values\nmax operation is used for numerical stability of the softmax especially keeping exp from exploding.\nHere next we will try to incorporated this stabilization technique in the above derived framework.\n\\[\\begin{align}\nm_0 = S^{[m, n=0]} \\\\\nm_{j+1} = \\max(m_j, S^{[m, n=j+1]}) \\\\\nl_0 = \\exp(S^{[m, n=0]} - m_0) \\\\\nl_{j+1} = \\sum_{n\\in[0 \\dots j, j+1]}\\exp(S^{[m, n]} - m_{j+1}) \\\\\nl_{j+1} = \\frac{exp(-m_{j})}{\\exp(m_{j+1} - m_{j})}\\sum_{n\\in[0 \\dots j]}\\exp(S^{[m, n]}) + \\exp(S^{[m, n=j+1]} - m_{j+1}) \\\\\nl_{j+1} = l_{j}\\exp(m_{j} - m_{j+1}) + \\exp(S^{[m, n=j+1]} - m_{j+1}) \\\\\nO^{[m, d]}_{0} = \\frac{\\exp(S^{[m, n=0]} - m_{0}) \\cdot V^{[n=0, d]}}{l_{0}} \\\\\nO^{[m, d]}_{j+1} = \\frac{O^{[m, d]}_{j} * l_j}{l_{j+1}} + \\frac{\\exp(S^{[m, n=j+1]} - m_{j+1}) \\cdot V^{[n=j+1, d]}}{l_{j+1}} \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "posts/flash-fwd-pll-14-4-2025-kernelized/index.html",
    "href": "posts/flash-fwd-pll-14-4-2025-kernelized/index.html",
    "title": "FlashAttention Kernel: Forward Pass (Parallelism)",
    "section": "",
    "text": "Continuing on my previous blog: FlashAttention Kernel: Forward Pass (MATH), here we will explore the possibility of parallelism in the Forward Pass Kernel with step by step code transform, and finally reaching a stage which is much closer to the CUDA programming model."
  },
  {
    "objectID": "posts/flash-fwd-pll-14-4-2025-kernelized/index.html#flash-attention-forward-pass",
    "href": "posts/flash-fwd-pll-14-4-2025-kernelized/index.html#flash-attention-forward-pass",
    "title": "FlashAttention Kernel: Forward Pass (Parallelism)",
    "section": "Flash Attention Forward Pass:",
    "text": "Flash Attention Forward Pass:\nIn my last blog we saw how math works in Flash Attention, and this was the final expression that we derived there:\n\\[\\begin{align}\nm_0 = S^{[m, n=0]} \\\\\nm_{j+1} = \\max(m_j, S^{[m, n=j+1]}) \\\\\nl_0 = \\exp(S^{[m, n=0]} - m_0) \\\\\nl_{j+1} = \\sum_{n\\in[0 \\dots j, j+1]}\\exp(S^{[m, n]} - m_{j+1}) \\\\\nl_{j+1} = \\frac{exp(-m_{j})}{\\exp(m_{j+1} - m_{j})}\\sum_{n\\in[0 \\dots j]}\\exp(S^{[m, n]}) + \\exp(S^{[m, n=j+1]} - m_{j+1}) \\\\\nl_{j+1} = l_{j}\\exp(m_{j} - m_{j+1}) + \\exp(S^{[m, n=j+1]} - m_{j+1}) \\\\\nO^{[m, d]}_{0} = \\frac{\\exp(S^{[m, n=0]} - m_{0}) \\cdot V^{[n=0, d]}}{l_{0}} \\\\\nO^{[m, d]}_{j+1} = \\frac{O^{[m, d]}_{j} * l_j}{l_{j+1}} + \\frac{\\exp(S^{[m, n=j+1]} - m_{j+1}) \\cdot V^{[n=j+1, d]}}{l_{j+1}} \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "posts/flash-fwd-pll-14-4-2025-kernelized/index.html#parallelization-analysis-forward-pass",
    "href": "posts/flash-fwd-pll-14-4-2025-kernelized/index.html#parallelization-analysis-forward-pass",
    "title": "FlashAttention Kernel: Forward Pass (Parallelism)",
    "section": "Parallelization Analysis: Forward Pass",
    "text": "Parallelization Analysis: Forward Pass\nUsing above mentioned math expressions for Flash Attention forward pass we can derive the following code (partly pseudo):\nfor i in range(0, M):\n    for j in range(0, N):\n        for d in range(0, D):\n            q_i = Q[i] # S1: No self dependency: [SLoop: i, TLoop: i]\n            k_j = K[j] # S2: No self dependency: [SLoop: j, TLoop: j]\n\n            m_i = m[i] # S3: No self dependency: [SLoop: i, TLoop: i]\n            l_i = l[i] # S4: No self dependency: [SLoop: i, TLoop: i]\n\n            S_ij = q_i @ k_j # S5: No self dependency: [SLoop: (i, j), TLoop: (i, j)] # Loop interchange possible # RAW on q_i, and k_j\n\n            m_ij = max(m_i, S_ij) # S6: No self dependency: [SLoop: (i, (i, j)), TLoop: (i, j)] # RAW on m_i and S_ij\n            l_ij = l_i * exp(m_i - m_ij) + exp(S_ij - m_ij) # S7: No self dependency: [SLoop: (j, (i, (i, j)), ((i, j), (i, j))), TLoop: (i, j)] # RAW on l_i, m_i, m_ij, S_ij, m_ij\n\n            o_id = O[i, d] # S8: No self dependency: [SLoop: (i, d), TLoop: (i, d)]\n            v_jd = V[j, d] # S9: No self dependency: [SLoop: (j, d), TLoop: (j, d)]\n\n            o_ijd = o_id * l_i / l_ij + exp(S_ij - m_ij) * v_jd / l_ij # S10: No self dependency: [SLoop: ((i, d), i, (i, j), (i, j), (i, j), (j, d), (i, j)), TLoop: (i, j, d)] # RAW on o_id, l_i, l_ij, S_ij, m_ij, v_jd, l_ij\n\n            ### Finally assign the results back to buffers\n            O[i, d] = o_ijd # S11: Aggregation over j # RAW on o_ijd\n            m[i] = m_ij # S12: Aggregation over j # RAW on m_ij\n            l[i] = l_ij # S13: Aggregation over j # RAW on l_ij\nThe code is annotated with comments that follows a simple information template:\n\nThe starting of the comments starts with letter S followed by a number, e.g. S2, this indicates an statement along with its given number, so S2 stands for statement 2.\nAfter statement number the comment lists of the statement if dependent on self, i.e. if the variable updated in the statement is further being updated in a future iteration.\nThe comment then follows a simple notation for loop iteration order, which consists of two parts Source Loop (SLoop) and Target Loop (TLoop). In case where a statement consists of multiple variables this entry will become a list of list, where each entry corresponds to following variable used in order. For a variable being used in the statement, the iteration state is denoted by either the loop variable i.e. i, j, or d in this particular case or if the variable is dependent upon multiple iteration loops then it will be a tuple of those variables e.g. (i, d).\nFollowing this the comment lists the type of dependency (either of WAW, RAW, and WAR) and corresponding elements to which that dependency applies.\nSometimes a comment can say something like Aggregation over j which means that the reduction is being performed over that particular loop variable. Though this is generally an inplace operation, for better segregation of dependencies update, and reuse of a variable this can be done by computing a source iteration dependent local variable e.g. o_ijd and then updating it to a variable that is independent of the target loop e.g. O[i, d] = o_ijd."
  },
  {
    "objectID": "posts/flash-fwd-pll-14-4-2025-kernelized/index.html#data-dependency-graph-ddg",
    "href": "posts/flash-fwd-pll-14-4-2025-kernelized/index.html#data-dependency-graph-ddg",
    "title": "FlashAttention Kernel: Forward Pass (Parallelism)",
    "section": "Data Dependency Graph (DDG)",
    "text": "Data Dependency Graph (DDG)\nThis graph is derived from the previously annotated code, and represent the data flow and dependency across different statements.\nHere we have mentioned SCC several times, which stands for Strongly Connected Component, it occurs in a graph if all the nodes of a sub-graph are accessible from every other node within that subgraph. In such case parallelization is not possible.\n\nNote: SCCs generally appears with Loop Carried Dependency which is denoted as LC-i/j in the code representing over which loop it appears. E.g. if it’s an LC-i the SCC could not be parallelized over loop i.\n\n\n\n\n\n\ngraph TD\n    S1[\"S1: Q[i]\"]\n    S2[\"S2: K[j]\"]\n    S3[\"S3: m[i]\"]\n    S4[\"S4: l[i]\"]\n    S5[\"S5: S_ij = f(q_i, k_j)\"]\n    S6[\"S6: m_ij = f(m_i, S_ij)\"]\n    S7[\"S7: l_ij = f(l_i, m_i, m_ij, S_ij, m_ij)\"]\n    S8[\"S8: O[i, d]\"]\n    S9[\"S9: V[j, d]\"]\n    S10[\"S10: o_ijd = f(o_id, l_i, l_ij, S_ij, m_ij, v_jd, l_ij)\"]\n    S11[\"S11: O[i, d] = o_ijd\"]\n    S12[\"S12: m[i] = m_ij\"]\n    S13[\"S13: l[i] = l_ij\"]\n\n    %% Intra-iteration dependencies\n    S1 --&gt; S5\n    S2 --&gt; S5\n    S5 --&gt; S6\n    S3 --&gt; S7\n    S5 --&gt; S7\n    S6 --&gt; S7\n    S4 --&gt; S10\n    S5 --&gt; S10\n    S6 --&gt; S10\n    S7 --&gt; S10\n    S9 --&gt; S10\n\n    %%  Force same level for {S11, S12, S13}\n    %% S11 ~~~ S12\n    %% S12 ~~~ S13\n\n    %% Strongly Connected Components (SCCs)\n    subgraph SCC1\n        S3 --&gt; S6\n        S6 --&gt; S12\n        S12 -.-&gt;|LC-j| S3\n    end\n\n    subgraph SCC2\n        S4 --&gt; S7\n        S7 --&gt; S13\n        S13 -.-&gt;|LC-j| S4\n    end\n\n    subgraph SCC3\n        S8 --&gt; S10\n        S10 --&gt; S11\n        S11 -.-&gt;|LC-j| S8\n    end\n\n\n\n\n\n\n\nDeductions from DDG:\n\nThere is no loop carries dependencies for i and d loop, so both are parallelizable.\nIf we analyze the loop order change all of the loop order i&lt;-&gt;j&lt;-&gt;d are valid, because the ony loop carried dependency is in j loop, and that has a positive lex i.e. source loop is j-1 and target loop is j.\nFor parallelization over j loop we need to localize dependent variables and utilize atomics to communicate across thread.\n\n\n\nLoop Interchange Analysis\nQuestion: If we could interchange the loop \\(i \\leftrightarrow j\\) to improve the locality of k_j and v_j as there are only 3 reads (q_i, K_j, & v_j) from HBM (excluding outputs and m_i & l_i).\nThe inner loop-j is responsible for loading both k_j and v_j from HBM and if the loops are interchanged a single load of k_j and v_j can be used for all of q_i, which when other way around is : a single load of q_i is being used for all k_j and v_j sequential loads.\nFor loop interchange one important factor is that “loop iteration dependence vector should not become lexicographically negative”\nExample:\nfor (i=1; i&lt;N; i++) {\n  for (j=1; j&lt;N; j++) {\n    A[i][j] = A[i-1][j+1]; // RAW dependencies on i and j\n  }\n}\nIn this loop the the direction vector of iteration for the one dependency i.e. A[i-1][j+1] is (1, -1). Which after switching the loops:\nfor (j=1; j&lt;N; j++) {\n  for (i=1; i&lt;N; i++) {\n    A[i][j] = ...\n  }\n}\nbecomes (-1, 1) which is called lexicographically negative, and thus doesn’t allows the loop interchange as the loop order would change if the interchange happens.\nSimply stating: in the original loop A[i-1][j+1] comes before A[i][j] and is updated before it. But after reorder A[i][j] will come before and modified than A[i-1][j+1] thus we can’t interchange the loops.\n\n\nModified Code: #1 Improved locality\nAs noted previously taking out some statements out of independent loop iterations increases variable locality, thus can be reused readily, reducing pressure on memory.\nfor i in range(0, M):\n    q_i = Q[i] # S1: No self dependency: [SLoop: i, TLoop: i]\n    m_i = m[i] # S3: No self dependency: [SLoop: i, TLoop: i]\n    l_i = l[i] # S4: No self dependency: [SLoop: i, TLoop: i]\n    for j in range(0, N):\n        k_j = K[j] # S2: No self dependency: [SLoop: j, TLoop: j]\n\n        S_ij = q_i @ k_j # S5: No self dependency: [SLoop: (i, j), TLoop: (i, j)] # Loop interchange possible # RAW on q_i, and k_j\n\n        m_ij = max(m_i, S_ij) # S6: No self dependency: [SLoop: (i, (i, j)), TLoop: (i, j)] # RAW on m_i and S_ij\n\n        l_ij = l_i * exp(m_i - m_ij) + exp(S_{ij} - m_ij) # S7: No self dependency: [SLoop: (j, (i, (i, j)), ((i, j), (i, j))), TLoop: (i, j)] # RAW on l_i, m_i, m_ij, S_ij, m_ij\n\n        for d in range(0, D):\n            o_id = O[i, d] # S8: No self dependency: [SLoop: (i, d), TLoop: (i, d)]\n            v_jd = V[j, d] # S9: No self dependency: [SLoop: (j, d), TLoop: (j, d)]\n\n            o_ijd = o_id * l_i / l_ij + exp(S_ij - m_ij) * v_jd / l_ij # S10: No self dependency: [SLoop: ((i, d), i, (i, j), (i, j), (i, j), (j, d), (i, j)), TLoop: (i, j, d)] # RAW on o_id, l_i, l_ij, S_ij, m_ij, v_jd, l_ij\n\n            ### Finally assign the results back to buffers\n            O[i, d] = o_ijd # S11: Aggregation over j # RAW on o_ijd\n\n    # Independent of `d` loop\n    m[i] = m_ij # S12: Aggregation over j # RAW on m_ij\n    l[i] = l_ij # S13: Aggregation over j # RAW on l_ij\n\n\nModified Code: #2 i & d loops full parallelization\nSince there are no self-dependencies in loop iterations i and d, these two loops can be fully parallelized.\n# Loop Collapsed under parallelization\n# for i in range(0, M):\nq_i = Q[i, :] # S1: No self dependency: [SLoop: i, TLoop: i]\nm_i = m[i] # S3: No self dependency: [SLoop: i, TLoop: i]\nl_i = l[i] # S4: No self dependency: [SLoop: i, TLoop: i]\no_i = O[i, :] # S8: No self dependency: [SLoop: (i, d), TLoop: (i, d)]\nfor j in range(0, N):\n    k_j = K[j, :] # S2: No self dependency: [SLoop: j, TLoop: j]\n    v_j = V[j, :] # S9: No self dependency: [SLoop: (j, d), TLoop: (j, d)]\n\n    S_ij = q_i @ k_j # S5: No self dependency: [SLoop: (i, j), TLoop: (i, j)] # Loop interchange possible # RAW on q_i, and k_j\n\n    m_ij = max(m_i, S_ij) # S6: No self dependency: [SLoop: (i, (i, j)), TLoop: (i, j)] # RAW on m_i and S_ij\n\n    l_ij = l_i * exp(m_i - m_ij) + exp(S_{ij} - m_ij) # S7: No self dependency: [SLoop: (j, (i, (i, j)), ((i, j), (i, j))), TLoop: (i, j)] # RAW on l_i, m_i, m_ij, S_ij, m_ij\n\n    # Loop Collapsed under parallelization\n    # for d in range(0, D):\n    o_ij = o_i * l_i / l_ij + exp(S_ij - m_ij) * v_j / l_ij # S10: No self dependency: [SLoop: ((i, d), i, (i, j), (i, j), (i, j), (j, d), (i, j)), TLoop: (i, j, d)] # RAW on o_id, l_i, l_ij, S_ij, m_ij, v_jd, l_ij\n\n    o_i = o_ij\n    m_i = m_ij\n    l_i = l_ij\n\n# Independent of `d` loop\nO[i] = o_i # S11: Aggregation over j # RAW on o_i\nm[i] = m_i # S12: Aggregation over j # RAW on m_i\nl[i] = l_i # S13: Aggregation over j # RAW on l_i\n\n\nModified Code: #3 attempt to parallelize loop j\nThough there are multiple SCCs with LC-j, still we can incorporate those in the CUDA programming model by launching all of the corresponding threads in one shot, while enforcing an order in which threads execute.\nThis is generally done with atomics in CUDA\n# assume global variable `counter = 0` which is incremented as per atomics.\n# Loop Collapsed under parallelization\n# for i in range(0, M):\nq_i = Q[i, :] # S1: No self dependency: [SLoop: i, TLoop: i]\n\n# Loop collapsed under parallelization\n# for j in range(0, N):\nk_j = K[j, :] # S2: No self dependency: [SLoop: j, TLoop: j]\nv_j = V[j, :] # S9: No self dependency: [SLoop: (j, d), TLoop: (j, d)]\n\nS_ij = q_i @ k_j # S5: No self dependency: [SLoop: (i, j), TLoop: (i, j)] # Loop interchange possible # RAW on q_i, and k_j\n\nwhile True:\n    if tid == counter:\n        m_i = m[i]\n        m_ij = max(m_i, S_ij) # S6: No self dependency: [SLoop: (i, (i, j)), TLoop: (i, j)] # RAW on m_i and S_ij\n        m[i] = m_ij\n        l_i = l[i]\n        l_ij = l_i * exp(m_i - m_ij) + exp(S_ij - m_ij)\n        l[i] = l_ij\n        o_i = O[i, :]\n        o_ij = o_i * l_i / l_ij + exp(S_ij - m_ij) * v_j / l_ij\n        O[i, :] = o_ij\n        atomicadd(counter, 1);\n        break\n\n\nModified Code: #3 attempt to parallelize loop j this time without enforcing an order\n# assume global variable `counter = 0` which is incremented as per atomics.\n# Loop Collapsed under parallelization\n# for i in range(0, M):\nq_i = Q[i, :] # S1: No self dependency: [SLoop: i, TLoop: i]\n\n# Loop collapsed under parallelization\n# for j in range(0, N):\nk_j = K[j, :] # S2: No self dependency: [SLoop: j, TLoop: j]\nv_j = V[j, :] # S9: No self dependency: [SLoop: (j, d), TLoop: (j, d)]\n\nS_ij = q_i @ k_j # S5: No self dependency: [SLoop: (i, j), TLoop: (i, j)] # Loop interchange possible # RAW on q_i, and k_j\n\n# Compute complete max till end\natomic-max(m[i], S_ij)\n\ngrid.sync()\n\natomic-add(l[i], exp(S_ij - m[i]))\n\ngrid.sync()\n\natomic-add(o[i], exp(S_ij - m[i]) * v_j / l[i])\n\ngrid.sync()\n\n\nModified Code: #4 attempt to parallelize loop j this time without enforcing an order\nAnother way to achieve this is Cooperative Thread Arrays (CTA) available in CUDA, more on this in next blog when we dive deep into CUDA code itself.\n# assume global variable `counter = 0` which is incremented as per atomics.\n# Loop Collapsed under parallelization\n# for i in range(0, M):\nq_i = Q[i, :] # S1: No self dependency: [SLoop: i, TLoop: i]\n\n# Loop collapsed under parallelization\n# for j in range(0, N):\nk_j = K[j, :] # S2: No self dependency: [SLoop: j, TLoop: j]\nv_j = V[j, :] # S9: No self dependency: [SLoop: (j, d), TLoop: (j, d)]\n\nS_ij = q_i @ k_j # S5: No self dependency: [SLoop: (i, j), TLoop: (i, j)] # Loop interchange possible # RAW on q_i, and k_j\n\n# Compute complete max till end\nmaxreductionthroughCTA(m[i], S_ij)\n\ngrid.sync()\n\naddreductionthroughCTA(l[i], exp(S_ij - m[i]))\n\ngrid.sync()\n\naddreductionthroughCTA(o[i], exp(S_ij - m[i]) * v_j / l[i])\n\ngrid.sync()\nWondering where is the code analysis for backward pass: Look at here:"
  }
]